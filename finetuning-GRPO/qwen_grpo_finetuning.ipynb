{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Qwen-0.5B with GRPO for Enhanced Reasoning\n",
    "\n",
    "This notebook demonstrates how to finetune the Qwen-0.5B model using GRPO (Grounded Preference Optimization) to enhance its reasoning capabilities.\n",
    "\n",
    "## Credits and References\n",
    "\n",
    "This implementation draws from several key sources:\n",
    "\n",
    "1. **GRPO Implementation**:\n",
    "   - [theLMbook's GRPO Implementation](https://github.com/aburkov/theLMbook/blob/main/GRPO_Qwen_0_5_Instruct.ipynb)\n",
    "   - [Unsloth's GRPO Documentation](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl)\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - [Qwen Paper](https://arxiv.org/abs/2309.16609)\n",
    "   - [Qwen Official Repository](https://github.com/QwenLM/Qwen)\n",
    "\n",
    "3. **Training Datasets**:\n",
    "   - [facebook/natural_reasoning](https://huggingface.co/datasets/facebook/natural_reasoning)\n",
    "   - [open-thoughts/OpenThoughts-114k](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n",
    "   - [SkunkworksAI/reasoning-0.01](https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01)\n",
    "\n",
    "4. **Additional Resources**:\n",
    "   - [PPO for Language Models](https://arxiv.org/abs/2109.10862)\n",
    "   - [TRL Library Documentation](https://huggingface.co/docs/trl/index)\n",
    "\n",
    "## Overview\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Base Model: Qwen-0.5B] --> B[Dataset Preparation]\n",
    "    B --> C[GRPO Training Setup]\n",
    "    C --> D[Training Loop]\n",
    "    D --> E[Reward Model]\n",
    "    E --> F[Model Updates]\n",
    "    F --> D\n",
    "    F --> G[Evaluation]\n",
    "```\n",
    "\n",
    "## Setup Requirements\n",
    "\n",
    "1. **Environment Setup**\n",
    "   ```bash\n",
    "   pip install transformers accelerate torch datasets wandb trl\n",
    "   ```\n",
    "\n",
    "2. **HuggingFace Authentication**\n",
    "   ```python\n",
    "   from huggingface_hub import login\n",
    "   login()  # Enter your token when prompted\n",
    "   ```\n",
    "\n",
    "3. **Hardware Requirements**\n",
    "   - GPU: NVIDIA GPU with 8GB+ VRAM\n",
    "   - RAM: 16GB+ recommended\n",
    "   - Storage: 10GB+ free space\n",
    "\n",
    "## Training Process\n",
    "![GRPO Training Process](https://raw.githubusercontent.com/unslothai/unsloth/main/docs/images/grpo.png)\n",
    "\n",
    "The training process involves:\n",
    "1. Generating responses to reasoning prompts\n",
    "2. Evaluating response quality using reward model\n",
    "3. Updating model weights using PPO\n",
    "4. Iterating with curriculum learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install -q transformers accelerate torch datasets wandb trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "We'll use Qwen-0.5B as our base model, following the approach from theLMbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model_name = \"Qwen/Qwen-0.5B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Training Data\n",
    "\n",
    "We'll combine multiple reasoning datasets to create a diverse training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and prepare reasoning datasets\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load multiple reasoning datasets\n",
    "datasets = {\n",
    "    \"natural_reasoning\": load_dataset(\"facebook/natural_reasoning\", split=\"train\"),\n",
    "    \"openthoughts\": load_dataset(\"open-thoughts/OpenThoughts-114k\", split=\"train\"),\n",
    "    \"skunkworks\": load_dataset(\"SkunkworksAI/reasoning-0.01\", split=\"train\")\n",
    "}\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"{name}: {len(dataset):,} examples\")\n",
    "\n",
    "def process_natural_reasoning(example):\n",
    "    \"\"\"Process facebook/natural_reasoning dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Question that requires reasoning\n",
    "    - Response: Step-by-step rationale\n",
    "    - Feedback: Quality assessment based on logical structure\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"question\"],\n",
    "        \"response\": f\"Let me solve this step by step:\\n{example['rationale']}\\n\\nTherefore, {example['answer']}\",\n",
    "        \"feedback\": \"Good reasoning with clear logical steps\" if len(example[\"rationale\"].split()) > 20 else \"Needs more detailed explanation\"\n",
    "    }\n",
    "\n",
    "def process_openthoughts(example):\n",
    "    \"\"\"Process OpenThoughts dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Open-ended prompt\n",
    "    - Response: Thought process and conclusion\n",
    "    - Feedback: Based on reasoning depth\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"prompt\"],\n",
    "        \"response\": f\"Let me think through this:\\n{example['thought_process']}\\n\\nConclusion: {example['response']}\",\n",
    "        \"feedback\": example.get(\"feedback\", \"Clear thought process with logical progression\")\n",
    "    }\n",
    "\n",
    "def process_skunkworks(example):\n",
    "    \"\"\"Process SkunkworksAI reasoning dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Reasoning task\n",
    "    - Response: Structured solution\n",
    "    - Feedback: Based on step-by-step approach\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"instruction\"],\n",
    "        \"response\": example[\"output\"],\n",
    "        \"feedback\": \"Excellent step-by-step reasoning\" if \"step\" in example[\"output\"].lower() else \"Could use more explicit steps\"\n",
    "    }\n",
    "\n",
    "print(\"\\nProcessing datasets...\")\n",
    "\n",
    "# Process datasets with progress tracking\n",
    "processed_datasets = {}\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    if name == \"natural_reasoning\":\n",
    "        processed_datasets[name] = dataset.map(process_natural_reasoning)\n",
    "    elif name == \"openthoughts\":\n",
    "        processed_datasets[name] = dataset.map(process_openthoughts)\n",
    "    else:\n",
    "        processed_datasets[name] = dataset.map(process_skunkworks)\n",
    "\n",
    "# Sample and combine datasets with balanced representation\n",
    "sample_sizes = {\n",
    "    \"natural_reasoning\": 50000,\n",
    "    \"openthoughts\": 30000,\n",
    "    \"skunkworks\": 20000\n",
    "}\n",
    "\n",
    "combined_dataset = concatenate_datasets([\n",
    "    processed_datasets[name].select(range(min(size, len(processed_datasets[name]))))\n",
    "    for name, size in sample_sizes.items()\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format example following Qwen's chat template.\n",
    "    \n",
    "    Based on: https://github.com/QwenLM/Qwen/blob/main/examples/finetune_chat.py\n",
    "    \"\"\"\n",
    "    if example[\"input\"]:\n",
    "        return f\"<|im_start|>user\\n{example['instruction']}\\n{example['input']}<|im_end|>\\n<|im_start|>assistant\\n{example['response']}<|im_end|>\"\n",
    "    return f\"<|im_start|>user\\n{example['instruction']}<|im_end|>\\n<|im_start|>assistant\\n{example['response']}<|im_end|>\"\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample formatted prompt:\")\n",
    "print(\"-\" * 80)\n",
    "print(format_prompt(combined_dataset[0]))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(combined_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Training Configuration\n",
    "\n",
    "Following the approach from theLMbook and Unsloth's GRPO implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from trl import PPOConfig, PPOTrainer\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=1.41e-5,  # From theLMbook optimal settings\n",
    "    ppo_epochs=4,\n",
    "    mini_batch_size=4,\n",
    "    batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optimize_cuda_cache=True\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=combined_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with Reward Model\n",
    "\n",
    "Implementing GRPO training with reward model based on reasoning quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_reasoning_quality(response):\n",
    "    \"\"\"Evaluate the quality of reasoning in a model response.\n",
    "    \n",
    "    Based on metrics from:\n",
    "    - theLMbook's GRPO implementation\n",
    "    - Unsloth's reasoning evaluation\n",
    "    \n",
    "    Metrics:\n",
    "    1. Step-by-step explanation (0.3)\n",
    "    2. Logical flow (0.2)\n",
    "    3. Depth of explanation (0.2)\n",
    "    4. Conclusion clarity (0.2)\n",
    "    5. Conciseness (0.1)\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"steps\": any(f\"{i}.\" in response for i in range(1, 10)),\n",
    "        \"logical_flow\": any(word in response.lower() for word in [\"because\", \"therefore\", \"since\", \"as a result\"]),\n",
    "        \"depth\": len(response.split()) >= 50,\n",
    "        \"conclusion\": any(word in response.lower() for word in [\"in conclusion\", \"therefore\", \"thus\", \"finally\"]),\n",
    "        \"concise\": len(response.split()) <= 200\n",
    "    }\n",
    "    \n",
    "    score = (\n",
    "        0.3 * int(metrics[\"steps\"]) +\n",
    "        0.2 * int(metrics[\"logical_flow\"]) +\n",
    "        0.2 * int(metrics[\"depth\"]) +\n",
    "        0.2 * int(metrics[\"conclusion\"]) +\n",
    "        0.1 * int(metrics[\"concise\"])\n",
    "    )\n",
    "    \n",
    "    return score, metrics\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"qwen-grpo-finetuning\", name=\"reasoning-enhancement\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(ppo_trainer.dataloader):\n",
    "        # Generate responses\n",
    "        query_tensors = tokenizer(batch[\"instruction\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "        response = ppo_trainer.generate(\n",
    "            query_tensors,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        \n",
    "        # Decode responses\n",
    "        response_texts = [tokenizer.decode(r, skip_special_tokens=True) for r in response]\n",
    "        \n",
    "        # Compute rewards\n",
    "        rewards = []\n",
    "        metrics_list = []\n",
    "        for r in response_texts:\n",
    "            score, metrics = compute_reasoning_quality(r)\n",
    "            rewards.append(score)\n",
    "            metrics_list.append(metrics)\n",
    "        \n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        \n",
    "        # PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response, rewards)\n",
    "        \n",
    "        # Log metrics\n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_metrics = {\n",
    "                \"steps_ratio\": sum(m[\"steps\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"logical_flow_ratio\": sum(m[\"logical_flow\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"depth_ratio\": sum(m[\"depth\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"conclusion_ratio\": sum(m[\"conclusion\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"concise_ratio\": sum(m[\"concise\"] for m in metrics_list) / len(metrics_list)\n",
    "            }\n",
    "            \n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": batch_idx,\n",
    "                \"mean_reward\": rewards.mean().item(),\n",
    "                **avg_metrics,\n",
    "                **stats\n",
    "            })\n",
    "            \n",
    "            print(f\"Batch {batch_idx}: Mean reward = {rewards.mean():.3f}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We'll evaluate the model on various reasoning tasks, following evaluation approaches from:\n",
    "- theLMbook's evaluation metrics\n",
    "- Unsloth's reasoning assessment\n",
    "- Standard reasoning benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test cases for different reasoning types\n",
    "test_cases = {\n",
    "    \"scientific\": [\n",
    "        \"Explain how photosynthesis works in plants, breaking down the process into steps.\",\n",
    "        \"Why do objects fall towards Earth? Explain using physics principles.\"\n",
    "    ],\n",
    "    \"mathematical\": [\n",
    "        \"Solve the equation: 2x + 5 = 15. Show your work step by step.\",\n",
    "        \"Calculate the area of a triangle with base 6 and height 8. Explain your reasoning.\"\n",
    "    ],\n",
    "    \"logical\": [\n",
    "        \"All birds have feathers. A penguin is a bird. What can we conclude about penguins?\",\n",
    "        \"If it's raining, the streets are wet. The streets are wet. Can we conclude it's raining?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "print(\"=== Model Evaluation Results ===\\n\")\n",
    "\n",
    "all_metrics = []\n",
    "for category, prompts in test_cases.items():\n",
    "    print(f\"\\n{category.upper()} REASONING TASKS:\\n\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Evaluate response\n",
    "        score, metrics = compute_reasoning_quality(response)\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"\\nResponse:\\n{response}\")\n",
    "        print(f\"\\nScore: {score:.2f}\")\n",
    "        print(f\"Metrics: {metrics}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Calculate overall statistics\n",
    "total_responses = len(all_metrics)\n",
    "overall_stats = {\n",
    "    \"step_by_step\": sum(1 for m in all_metrics if m[\"steps\"]) / total_responses * 100,\n",
    "    \"reasoning_markers\": sum(1 for m in all_metrics if m[\"logical_flow\"]) / total_responses * 100,\n",
    "    \"depth\": sum(1 for m in all_metrics if m[\"depth\"]) / total_responses * 100,\n",
    "    \"clear_conclusions\": sum(1 for m in all_metrics if m[\"conclusion\"]) / total_responses * 100,\n",
    "    \"conciseness\": sum(1 for m in all_metrics if m[\"concise\"]) / total_responses * 100\n",
    "}\n",
    "\n",
    "print(\"\\n=== Overall Statistics ===\")\n",
    "for metric, value in overall_stats.items():\n",
    "    print(f\"{metric}: {value:.1f}%\")\n",
    "\n",
    "# Save the model and evaluation results\n",
    "output_dir = \"qwen-grpo-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save evaluation results\n",
    "import json\n",
    "with open(f\"{output_dir}/evaluation_results.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"test_cases\": test_cases,\n",
    "        \"overall_stats\": overall_stats\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nModel and evaluation results saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}