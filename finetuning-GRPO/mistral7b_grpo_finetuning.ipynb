{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Mistral-7B with GRPO for Better Reasoning\n",
    "\n",
    "This notebook demonstrates how to finetune Mistral-7B using GRPO (Grounded Preference Optimization) to enhance its reasoning capabilities.\n",
    "\n",
    "## Overview\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Base Model: Mistral-7B] --> B[Quantization & LoRA Setup]\n",
    "    B --> C[Load Reasoning Datasets]\n",
    "    C --> D[GRPO Training Loop]\n",
    "    D --> E[Reward Model]\n",
    "    E --> F[Model Updates]\n",
    "    F --> D\n",
    "    F --> G[Final Finetuned Model]\n",
    "```\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **HuggingFace Token Setup**\n",
    "   ```python\n",
    "   from huggingface_hub import login\n",
    "   login()  # Enter your token when prompted\n",
    "   ```\n",
    "   Get your token from: https://huggingface.co/settings/tokens\n",
    "\n",
    "2. **Required GPU**: This notebook is designed for Brev.dev GPUs\n",
    "   - Minimum requirements: 16GB VRAM\n",
    "   - Recommended: A100 or similar\n",
    "\n",
    "3. **Datasets Used**:\n",
    "   - [facebook/natural_reasoning](https://huggingface.co/datasets/facebook/natural_reasoning)\n",
    "   - [SkunkworksAI/reasoning-0.01](https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01)\n",
    "   - [open-thoughts/OpenThoughts-114k](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n",
    "\n",
    "## GRPO Architecture\n",
    "![GRPO Architecture](https://raw.githubusercontent.com/unslothai/unsloth/main/docs/images/grpo.png)\n",
    "\n",
    "## Key Components\n",
    "1. **Quantization**: 4-bit precision for efficient training\n",
    "2. **LoRA**: Low-Rank Adaptation for parameter-efficient finetuning\n",
    "3. **GRPO**: Combines preference optimization with grounded feedback\n",
    "4. **Reward Model**: Evaluates reasoning quality and logical coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install -q transformers accelerate bitsandbytes datasets torch peft trl wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "def print_gpu_memory():\n",
    "    \"\"\"Print GPU memory usage.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nGPU Memory Usage:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i}: {torch.cuda.get_device_properties(i).name}\")\n",
    "            print(f\"  Allocated: {torch.cuda.memory_allocated(i) / 1024**2:.1f}MB\")\n",
    "            print(f\"  Cached: {torch.cuda.memory_reserved(i) / 1024**2:.1f}MB\")\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "\n",
    "print_gpu_memory()\n",
    "import wandb\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Configure quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and prepare reasoning datasets\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load multiple reasoning datasets\n",
    "datasets = {\n",
    "    \"natural_reasoning\": load_dataset(\"facebook/natural_reasoning\", split=\"train\"),\n",
    "    \"openthoughts\": load_dataset(\"open-thoughts/OpenThoughts-114k\", split=\"train\"),\n",
    "    \"skunkworks\": load_dataset(\"SkunkworksAI/reasoning-0.01\", split=\"train\")\n",
    "}\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"{name}: {len(dataset):,} examples\")\n",
    "\n",
    "def process_natural_reasoning(example):\n",
    "    \"\"\"Process facebook/natural_reasoning dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Question that requires reasoning\n",
    "    - Response: Step-by-step rationale\n",
    "    - Feedback: Quality assessment based on logical structure\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"question\"],\n",
    "        \"response\": f\"Let me solve this step by step:\\n{example['rationale']}\\n\\nTherefore, {example['answer']}\",\n",
    "        \"feedback\": \"Good reasoning with clear logical steps\" if len(example[\"rationale\"].split()) > 20 else \"Needs more detailed explanation\"\n",
    "    }\n",
    "\n",
    "def process_openthoughts(example):\n",
    "    \"\"\"Process OpenThoughts dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Open-ended prompt\n",
    "    - Response: Thought process and conclusion\n",
    "    - Feedback: Based on reasoning depth\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"prompt\"],\n",
    "        \"response\": f\"Let me think through this:\\n{example['thought_process']}\\n\\nConclusion: {example['response']}\",\n",
    "        \"feedback\": example.get(\"feedback\", \"Clear thought process with logical progression\")\n",
    "    }\n",
    "\n",
    "def process_skunkworks(example):\n",
    "    \"\"\"Process SkunkworksAI reasoning dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Reasoning task\n",
    "    - Response: Structured solution\n",
    "    - Feedback: Based on step-by-step approach\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"instruction\"],\n",
    "        \"response\": example[\"output\"],\n",
    "        \"feedback\": \"Excellent step-by-step reasoning\" if \"step\" in example[\"output\"].lower() else \"Could use more explicit steps\"\n",
    "    }\n",
    "\n",
    "print(\"\\nProcessing datasets...\")\n",
    "\n",
    "# Process datasets with progress tracking\n",
    "processed_datasets = {}\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    if name == \"natural_reasoning\":\n",
    "        processed_datasets[name] = dataset.map(process_natural_reasoning)\n",
    "    elif name == \"openthoughts\":\n",
    "        processed_datasets[name] = dataset.map(process_openthoughts)\n",
    "    else:\n",
    "        processed_datasets[name] = dataset.map(process_skunkworks)\n",
    "\n",
    "# Sample and combine datasets with balanced representation\n",
    "sample_sizes = {\n",
    "    \"natural_reasoning\": 50000,\n",
    "    \"openthoughts\": 30000,\n",
    "    \"skunkworks\": 20000\n",
    "}\n",
    "\n",
    "combined_dataset = concatenate_datasets([\n",
    "    processed_datasets[name].select(range(min(size, len(processed_datasets[name]))))\n",
    "    for name, size in sample_sizes.items()\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format example for Mistral instruction format.\n",
    "    \n",
    "    Structure:\n",
    "    1. System prompt for reasoning task\n",
    "    2. User instruction\n",
    "    3. Assistant response with reasoning\n",
    "    4. Feedback for grounding\n",
    "    \"\"\"\n",
    "    return f\"<s>[INST] {example['instruction']} [/INST] {example['response']}\\n\\nFeedback: {example['feedback']}</s>\"\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample formatted prompt:\")\n",
    "print(\"-\" * 80)\n",
    "print(format_prompt(combined_dataset[0]))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(combined_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=8,\n",
    "    mini_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optimize_cuda_cache=True\n",
    ")\n",
    "\n",
    "# Initialize PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=combined_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_reasoning_quality(response):\n",
    "    \"\"\"Evaluate the quality of reasoning in a model response.\n",
    "    \n",
    "    Metrics:\n",
    "    1. Step-by-step explanation (0.3)\n",
    "    2. Logical flow (0.2)\n",
    "    3. Depth of explanation (0.2)\n",
    "    4. Conclusion clarity (0.2)\n",
    "    5. Conciseness (0.1)\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"steps\": any(f\"{i}.\" in response for i in range(1, 10)),\n",
    "        \"logical_flow\": any(word in response.lower() for word in [\"because\", \"therefore\", \"since\", \"as a result\"]),\n",
    "        \"depth\": len(response.split()) >= 50,\n",
    "        \"conclusion\": any(word in response.lower() for word in [\"in conclusion\", \"therefore\", \"thus\", \"finally\"]),\n",
    "        \"concise\": len(response.split()) <= 200\n",
    "    }\n",
    "    \n",
    "    score = (\n",
    "        0.3 * int(metrics[\"steps\"]) +\n",
    "        0.2 * int(metrics[\"logical_flow\"]) +\n",
    "        0.2 * int(metrics[\"depth\"]) +\n",
    "        0.2 * int(metrics[\"conclusion\"]) +\n",
    "        0.1 * int(metrics[\"concise\"])\n",
    "    )\n",
    "    \n",
    "    return score, metrics\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"mistral-grpo-finetuning\", name=\"reasoning-enhancement\")\n",
    "\n",
    "# Memory optimization tips:\n",
    "# 1. Monitor GPU memory usage throughout training\n",
    "# 2. Adjust batch size and gradient accumulation based on available memory\n",
    "# 3. Use gradient checkpointing for larger models\n",
    "# 4. Clear GPU cache between epochs if needed\n",
    "\n",
    "print(\"\\nInitial GPU memory state:\")\n",
    "print_gpu_memory()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    # Clear GPU cache between epochs\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(ppo_trainer.dataloader):\n",
    "        # Generate responses\n",
    "        query_tensors = tokenizer(batch[\"instruction\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "        response = ppo_trainer.generate(\n",
    "            query_tensors,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        \n",
    "        # Decode responses\n",
    "        response_texts = [tokenizer.decode(r, skip_special_tokens=True) for r in response]\n",
    "        \n",
    "        # Compute rewards\n",
    "        rewards = []\n",
    "        metrics_list = []\n",
    "        for r in response_texts:\n",
    "            score, metrics = compute_reasoning_quality(r)\n",
    "            rewards.append(score)\n",
    "            metrics_list.append(metrics)\n",
    "        \n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        \n",
    "        # PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response, rewards)\n",
    "        \n",
    "        # Log metrics\n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_metrics = {\n",
    "                \"steps_ratio\": sum(m[\"steps\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"logical_flow_ratio\": sum(m[\"logical_flow\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"depth_ratio\": sum(m[\"depth\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"conclusion_ratio\": sum(m[\"conclusion\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"concise_ratio\": sum(m[\"concise\"] for m in metrics_list) / len(metrics_list)\n",
    "            }\n",
    "            \n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": batch_idx,\n",
    "                \"mean_reward\": rewards.mean().item(),\n",
    "                **avg_metrics,\n",
    "                **stats\n",
    "            })\n",
    "            \n",
    "            print(f\"Batch {batch_idx}: Mean reward = {rewards.mean():.3f}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test cases for different reasoning types\n",
    "test_cases = {\n",
    "    \"scientific\": [\n",
    "        \"Explain why ice floats on water using molecular principles.\",\n",
    "        \"How does the greenhouse effect work? Explain the process.\"\n",
    "    ],\n",
    "    \"mathematical\": [\n",
    "        \"If a rectangle has length 8 and width 6, what is its area and perimeter? Show your work.\",\n",
    "        \"Solve: 3x - 7 = 14. Explain each step of your solution.\"\n",
    "    ],\n",
    "    \"logical\": [\n",
    "        \"All mammals are warm-blooded. Dolphins are mammals. What can we conclude about dolphins?\",\n",
    "        \"If it's sunny, Alice goes for a walk. Alice didn't go for a walk today. What can we conclude?\"\n",
    "    ],\n",
    "    \"causal\": [\n",
    "        \"Why do leaves change color in autumn? Explain the causal chain.\",\n",
    "        \"How does lack of sleep affect cognitive performance? Describe the mechanisms.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "print(\"=== Model Evaluation Results ===\\n\")\n",
    "\n",
    "all_metrics = []\n",
    "for category, prompts in test_cases.items():\n",
    "    print(f\"\\n{category.upper()} REASONING TASKS:\\n\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Evaluate response\n",
    "        score, metrics = compute_reasoning_quality(response)\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"\\nResponse:\\n{response}\")\n",
    "        print(f\"\\nScore: {score:.2f}\")\n",
    "        print(f\"Metrics: {metrics}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Calculate overall statistics\n",
    "total_responses = len(all_metrics)\n",
    "overall_stats = {\n",
    "    \"step_by_step\": sum(1 for m in all_metrics if m[\"steps\"]) / total_responses * 100,\n",
    "    \"reasoning_markers\": sum(1 for m in all_metrics if m[\"logical_flow\"]) / total_responses * 100,\n",
    "    \"depth\": sum(1 for m in all_metrics if m[\"depth\"]) / total_responses * 100,\n",
    "    \"clear_conclusions\": sum(1 for m in all_metrics if m[\"conclusion\"]) / total_responses * 100,\n",
    "    \"conciseness\": sum(1 for m in all_metrics if m[\"concise\"]) / total_responses * 100\n",
    "}\n",
    "\n",
    "print(\"\\n=== Overall Statistics ===\")\n",
    "for metric, value in overall_stats.items():\n",
    "    print(f\"{metric}: {value:.1f}%\")\n",
    "\n",
    "# Save the model and evaluation results\n",
    "output_dir = \"mistral7b-grpo-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save evaluation results\n",
    "import json\n",
    "with open(f\"{output_dir}/evaluation_results.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"test_cases\": test_cases,\n",
    "        \"overall_stats\": overall_stats\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nModel and evaluation results saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}