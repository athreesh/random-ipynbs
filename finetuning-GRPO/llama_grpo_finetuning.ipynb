{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Llama-2 1B with GRPO for Reasoning\n",
    "\n",
    "This notebook demonstrates how to finetune Llama-2 1B using GRPO (Grounded Preference Optimization) to enhance its reasoning capabilities.\n",
    "\n",
    "## Credits and References\n",
    "\n",
    "This implementation draws from several key sources:\n",
    "\n",
    "1. **GRPO Implementation**:\n",
    "   - [theLMbook's GRPO Implementation](https://github.com/aburkov/theLMbook/blob/main/GRPO_Qwen_0_5_Instruct.ipynb)\n",
    "   - [Unsloth's GRPO Documentation](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl)\n",
    "   - [DeepSeek-R1 Paper](https://thelmbook.com/articles/#!./DeepSeek-R1.md)\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - [Llama 2 Paper](https://arxiv.org/abs/2307.09288)\n",
    "   - [Llama 2 Official Repository](https://github.com/facebookresearch/llama)\n",
    "   - [Parameter-Efficient Fine-tuning Guide](https://huggingface.co/docs/peft/index)\n",
    "\n",
    "3. **Training Datasets**:\n",
    "   - [facebook/natural_reasoning](https://huggingface.co/datasets/facebook/natural_reasoning)\n",
    "   - [open-thoughts/OpenThoughts-114k](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n",
    "   - [SkunkworksAI/reasoning-0.01](https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01)\n",
    "\n",
    "4. **Additional Resources**:\n",
    "   - [PPO for Language Models](https://arxiv.org/abs/2109.10862)\n",
    "   - [TRL Library Documentation](https://huggingface.co/docs/trl/index)\n",
    "   - [LoRA: Low-Rank Adaptation Paper](https://arxiv.org/abs/2106.09685)\n",
    "\n",
    "## Overview\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Llama-2 1B Base Model] --> B[4-bit Quantization]\n",
    "    B --> C[LoRA Adaptation]\n",
    "    C --> D[Dataset Preparation]\n",
    "    D --> E[GRPO Training]\n",
    "    E --> F[Reward Model]\n",
    "    F --> G[Model Updates]\n",
    "    G --> E\n",
    "    G --> H[Final Model]\n",
    "```\n",
    "\n",
    "## Setup Requirements\n",
    "\n",
    "1. **Environment Setup**\n",
    "   ```bash\n",
    "   pip install -q transformers accelerate bitsandbytes datasets torch peft trl wandb\n",
    "   ```\n",
    "\n",
    "2. **HuggingFace Authentication**\n",
    "   ```python\n",
    "   from huggingface_hub import login\n",
    "   login()  # Enter your token when prompted\n",
    "   ```\n",
    "\n",
    "3. **Hardware Requirements**\n",
    "   - GPU: NVIDIA GPU with 8GB+ VRAM\n",
    "   - RAM: 16GB+ recommended\n",
    "   - Storage: 5GB+ free space\n",
    "\n",
    "## Training Process\n",
    "![GRPO Training Process](https://raw.githubusercontent.com/unslothai/unsloth/main/docs/images/grpo.png)\n",
    "\n",
    "The training process involves:\n",
    "1. Loading and preprocessing reasoning datasets\n",
    "2. Applying 4-bit quantization and LoRA\n",
    "3. Training with GRPO and reward model\n",
    "4. Evaluating reasoning capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install -q transformers accelerate bitsandbytes datasets torch peft trl wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import wandb\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "We'll use Llama-2 1B as our base model, with 4-bit quantization for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "model_name = \"meta-llama/Llama-2-1b-hf\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LoRA for Efficient Training\n",
    "\n",
    "Using LoRA for parameter-efficient fine-tuning, following the approach from the LoRA paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of update matrices\n",
    "    lora_alpha=16,  # Alpha scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention modules to adapt\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Training Data\n",
    "\n",
    "We'll combine multiple reasoning datasets to create a diverse training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and prepare reasoning datasets\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load multiple reasoning datasets\n",
    "datasets = {\n",
    "    \"natural_reasoning\": load_dataset(\"facebook/natural_reasoning\", split=\"train\"),\n",
    "    \"openthoughts\": load_dataset(\"open-thoughts/OpenThoughts-114k\", split=\"train\"),\n",
    "    \"skunkworks\": load_dataset(\"SkunkworksAI/reasoning-0.01\", split=\"train\")\n",
    "}\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"{name}: {len(dataset):,} examples\")\n",
    "\n",
    "def process_natural_reasoning(example):\n",
    "    \"\"\"Process facebook/natural_reasoning dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Question that requires reasoning\n",
    "    - Response: Step-by-step rationale\n",
    "    - Feedback: Quality assessment based on logical structure\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"question\"],\n",
    "        \"response\": f\"Let me solve this step by step:\\n{example['rationale']}\\n\\nTherefore, {example['answer']}\",\n",
    "        \"feedback\": \"Good reasoning with clear logical steps\" if len(example[\"rationale\"].split()) > 20 else \"Needs more detailed explanation\"\n",
    "    }\n",
    "\n",
    "def process_openthoughts(example):\n",
    "    \"\"\"Process OpenThoughts dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Open-ended prompt\n",
    "    - Response: Thought process and conclusion\n",
    "    - Feedback: Based on reasoning depth\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"prompt\"],\n",
    "        \"response\": f\"Let me think through this:\\n{example['thought_process']}\\n\\nConclusion: {example['response']}\",\n",
    "        \"feedback\": example.get(\"feedback\", \"Clear thought process with logical progression\")\n",
    "    }\n",
    "\n",
    "def process_skunkworks(example):\n",
    "    \"\"\"Process SkunkworksAI reasoning dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Reasoning task\n",
    "    - Response: Structured solution\n",
    "    - Feedback: Based on step-by-step approach\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"instruction\"],\n",
    "        \"response\": example[\"output\"],\n",
    "        \"feedback\": \"Excellent step-by-step reasoning\" if \"step\" in example[\"output\"].lower() else \"Could use more explicit steps\"\n",
    "    }\n",
    "\n",
    "print(\"\\nProcessing datasets...\")\n",
    "\n",
    "# Process datasets with progress tracking\n",
    "processed_datasets = {}\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    if name == \"natural_reasoning\":\n",
    "        processed_datasets[name] = dataset.map(process_natural_reasoning)\n",
    "    elif name == \"openthoughts\":\n",
    "        processed_datasets[name] = dataset.map(process_openthoughts)\n",
    "    else:\n",
    "        processed_datasets[name] = dataset.map(process_skunkworks)\n",
    "\n",
    "# Sample and combine datasets with balanced representation\n",
    "sample_sizes = {\n",
    "    \"natural_reasoning\": 50000,\n",
    "    \"openthoughts\": 30000,\n",
    "    \"skunkworks\": 20000\n",
    "}\n",
    "\n",
    "combined_dataset = concatenate_datasets([\n",
    "    processed_datasets[name].select(range(min(size, len(processed_datasets[name]))))\n",
    "    for name, size in sample_sizes.items()\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format example for Llama-2 instruction format.\n",
    "    \n",
    "    Structure:\n",
    "    1. System prompt for reasoning task\n",
    "    2. User instruction\n",
    "    3. Assistant response with reasoning\n",
    "    4. Feedback for grounding\n",
    "    \"\"\"\n",
    "    return f\"[INST] <<SYS>>\\nYou are a helpful AI assistant that provides clear, step-by-step reasoning for questions.\\n<</SYS>>\\n\\n{example['instruction']} [/INST]\\n{example['response']}\\n\\nFeedback: {example['feedback']}\"\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample formatted prompt:\")\n",
    "print(\"-\" * 80)\n",
    "print(format_prompt(combined_dataset[0]))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(combined_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Training Setup\n",
    "\n",
    "Following the approach from theLMbook and Unsloth's GRPO implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from trl import PPOTrainer, PPOConfig\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=4,\n",
    "    mini_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optimize_cuda_cache=True\n",
    ")\n",
    "\n",
    "# Initialize PPO trainer\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=combined_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with Reward Model\n",
    "\n",
    "Implementing GRPO training with reward model based on reasoning quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_reasoning_quality(response):\n",
    "    \"\"\"Evaluate the quality of reasoning in a model response.\n",
    "    \n",
    "    Based on metrics from:\n",
    "    - theLMbook's GRPO implementation\n",
    "    - Unsloth's reasoning evaluation\n",
    "    - DeepSeek-R1 paper\n",
    "    \n",
    "    Metrics:\n",
    "    1. Step-by-step explanation (0.3)\n",
    "    2. Logical flow (0.2)\n",
    "    3. Depth of explanation (0.2)\n",
    "    4. Conclusion clarity (0.2)\n",
    "    5. Conciseness (0.1)\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        \"steps\": any(f\"{i}.\" in response for i in range(1, 10)),\n",
    "        \"logical_flow\": any(word in response.lower() for word in [\"because\", \"therefore\", \"since\", \"as a result\"]),\n",
    "        \"depth\": len(response.split()) >= 50,\n",
    "        \"conclusion\": any(word in response.lower() for word in [\"in conclusion\", \"therefore\", \"thus\", \"finally\"]),\n",
    "        \"concise\": len(response.split()) <= 200\n",
    "    }\n",
    "    \n",
    "    score = (\n",
    "        0.3 * int(metrics[\"steps\"]) +\n",
    "        0.2 * int(metrics[\"logical_flow\"]) +\n",
    "        0.2 * int(metrics[\"depth\"]) +\n",
    "        0.2 * int(metrics[\"conclusion\"]) +\n",
    "        0.1 * int(metrics[\"concise\"])\n",
    "    )\n",
    "    \n",
    "    return score, metrics\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"llama-grpo-finetuning\", name=\"reasoning-enhancement\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(ppo_trainer.dataloader):\n",
    "        # Generate responses\n",
    "        query_tensors = tokenizer(batch[\"instruction\"], return_tensors=\"pt\", padding=True).to(device)\n",
    "        response = ppo_trainer.generate(\n",
    "            query_tensors,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        \n",
    "        # Decode responses\n",
    "        response_texts = [tokenizer.decode(r, skip_special_tokens=True) for r in response]\n",
    "        \n",
    "        # Compute rewards\n",
    "        rewards = []\n",
    "        metrics_list = []\n",
    "        for r in response_texts:\n",
    "            score, metrics = compute_reasoning_quality(r)\n",
    "            rewards.append(score)\n",
    "            metrics_list.append(metrics)\n",
    "        \n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        \n",
    "        # PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response, rewards)\n",
    "        \n",
    "        # Log metrics\n",
    "        if batch_idx % 10 == 0:\n",
    "            avg_metrics = {\n",
    "                \"steps_ratio\": sum(m[\"steps\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"logical_flow_ratio\": sum(m[\"logical_flow\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"depth_ratio\": sum(m[\"depth\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"conclusion_ratio\": sum(m[\"conclusion\"] for m in metrics_list) / len(metrics_list),\n",
    "                \"concise_ratio\": sum(m[\"concise\"] for m in metrics_list) / len(metrics_list)\n",
    "            }\n",
    "            \n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": batch_idx,\n",
    "                \"mean_reward\": rewards.mean().item(),\n",
    "                **avg_metrics,\n",
    "                **stats\n",
    "            })\n",
    "            \n",
    "            print(f\"Batch {batch_idx}: Mean reward = {rewards.mean():.3f}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We'll evaluate the model on various reasoning tasks, following evaluation approaches from:\n",
    "- theLMbook's evaluation metrics\n",
    "- Unsloth's reasoning assessment\n",
    "- DeepSeek-R1 paper benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test cases for different reasoning types\n",
    "test_cases = {\n",
    "    \"scientific\": [\n",
    "        \"Explain why ice floats on water using molecular principles.\",\n",
    "        \"How does the greenhouse effect work? Explain the process.\"\n",
    "    ],\n",
    "    \"mathematical\": [\n",
    "        \"If a rectangle has length 8 and width 6, what is its area and perimeter? Show your work.\",\n",
    "        \"Solve: 3x - 7 = 14. Explain each step of your solution.\"\n",
    "    ],\n",
    "    \"logical\": [\n",
    "        \"All mammals are warm-blooded. Dolphins are mammals. What can we conclude about dolphins?\",\n",
    "        \"If it's sunny, Alice goes for a walk. Alice didn't go for a walk today. What can we conclude?\"\n",
    "    ],\n",
    "    \"causal\": [\n",
    "        \"Why do leaves change color in autumn? Explain the causal chain.\",\n",
    "        \"How does lack of sleep affect cognitive performance? Describe the mechanisms.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "print(\"=== Model Evaluation Results ===\\n\")\n",
    "\n",
    "all_metrics = []\n",
    "for category, prompts in test_cases.items():\n",
    "    print(f\"\\n{category.upper()} REASONING TASKS:\\n\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Evaluate response\n",
    "        score, metrics = compute_reasoning_quality(response)\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"\\nResponse:\\n{response}\")\n",
    "        print(f\"\\nScore: {score:.2f}\")\n",
    "        print(f\"Metrics: {metrics}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Calculate overall statistics\n",
    "total_responses = len(all_metrics)\n",
    "overall_stats = {\n",
    "    \"step_by_step\": sum(1 for m in all_metrics if m[\"steps\"]) / total_responses * 100,\n",
    "    \"reasoning_markers\": sum(1 for m in all_metrics if m[\"logical_flow\"]) / total_responses * 100,\n",
    "    \"depth\": sum(1 for m in all_metrics if m[\"depth\"]) / total_responses * 100,\n",
    "    \"clear_conclusions\": sum(1 for m in all_metrics if m[\"conclusion\"]) / total_responses * 100,\n",
    "    \"conciseness\": sum(1 for m in all_metrics if m[\"concise\"]) / total_responses * 100\n",
    "}\n",
    "\n",
    "print(\"\\n=== Overall Statistics ===\")\n",
    "for metric, value in overall_stats.items():\n",
    "    print(f\"{metric}: {value:.1f}%\")\n",
    "\n",
    "# Save the model and evaluation results\n",
    "output_dir = \"llama-1b-grpo-finetuned\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Save evaluation results\n",
    "import json\n",
    "with open(f\"{output_dir}/evaluation_results.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"test_cases\": test_cases,\n",
    "        \"overall_stats\": overall_stats\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nModel and evaluation results saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}