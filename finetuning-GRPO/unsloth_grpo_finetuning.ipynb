{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning LLMs with Unsloth GRPO\n",
    "\n",
    "This notebook demonstrates how to use Unsloth's implementation of GRPO (Grounded Preference Optimization) to enhance reasoning capabilities in LLMs.\n",
    "\n",
    "## Credits and References\n",
    "\n",
    "This implementation is based on several key sources:\n",
    "\n",
    "1. **Unsloth Framework**:\n",
    "   - [Unsloth Documentation](https://docs.unsloth.ai/)\n",
    "   - [Unsloth GRPO Guide](https://docs.unsloth.ai/basics/reasoning-grpo-and-rl)\n",
    "   - [Unsloth GitHub Repository](https://github.com/unslothai/unsloth)\n",
    "\n",
    "2. **GRPO Implementation**:\n",
    "   - [theLMbook's GRPO Implementation](https://github.com/aburkov/theLMbook/blob/main/GRPO_Qwen_0_5_Instruct.ipynb)\n",
    "   - [DeepSeek-R1 Paper](https://thelmbook.com/articles/#!./DeepSeek-R1.md)\n",
    "\n",
    "3. **Training Datasets**:\n",
    "   - [facebook/natural_reasoning](https://huggingface.co/datasets/facebook/natural_reasoning)\n",
    "   - [open-thoughts/OpenThoughts-114k](https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k)\n",
    "   - [SkunkworksAI/reasoning-0.01](https://huggingface.co/datasets/SkunkworksAI/reasoning-0.01)\n",
    "\n",
    "4. **Additional Resources**:\n",
    "   - [PPO for Language Models](https://arxiv.org/abs/2109.10862)\n",
    "   - [LoRA: Low-Rank Adaptation Paper](https://arxiv.org/abs/2106.09685)\n",
    "   - [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
    "\n",
    "## Overview\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Base LLM] --> B[Unsloth Optimization]\n",
    "    B --> C[Dataset Preparation]\n",
    "    C --> D[GRPO Training]\n",
    "    D --> E[Reward Model]\n",
    "    E --> F[Model Updates]\n",
    "    F --> D\n",
    "    F --> G[Final Model]\n",
    "```\n",
    "\n",
    "## Setup Requirements\n",
    "\n",
    "1. **Environment Setup**\n",
    "   ```bash\n",
    "   pip install unsloth accelerate bitsandbytes datasets torch wandb\n",
    "   ```\n",
    "\n",
    "2. **HuggingFace Authentication**\n",
    "   ```python\n",
    "   from huggingface_hub import login\n",
    "   login()  # Enter your token when prompted\n",
    "   ```\n",
    "\n",
    "3. **Hardware Requirements**\n",
    "   - GPU: NVIDIA GPU with 16GB+ VRAM\n",
    "   - RAM: 32GB+ recommended\n",
    "   - Storage: 20GB+ free space\n",
    "\n",
    "## Training Process\n",
    "![GRPO Training Process](https://raw.githubusercontent.com/unslothai/unsloth/main/docs/images/grpo.png)\n",
    "\n",
    "The training process involves:\n",
    "1. Loading and preprocessing reasoning datasets\n",
    "2. Applying Unsloth optimizations\n",
    "3. Training with GRPO and reward model\n",
    "4. Evaluating reasoning capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install -q unsloth accelerate bitsandbytes datasets torch wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import TrainingArguments\n",
    "import wandb\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model with Unsloth Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize model with Unsloth optimizations\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # Can be changed to other models\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,  # defaults to best dtype\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and prepare reasoning datasets\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import random\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Load multiple reasoning datasets\n",
    "datasets = {\n",
    "    \"natural_reasoning\": load_dataset(\"facebook/natural_reasoning\", split=\"train\"),\n",
    "    \"openthoughts\": load_dataset(\"open-thoughts/OpenThoughts-114k\", split=\"train\"),\n",
    "    \"skunkworks\": load_dataset(\"SkunkworksAI/reasoning-0.01\", split=\"train\")\n",
    "}\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"{name}: {len(dataset):,} examples\")\n",
    "\n",
    "def process_natural_reasoning(example):\n",
    "    \"\"\"Process facebook/natural_reasoning dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Question that requires reasoning\n",
    "    - Response: Step-by-step rationale\n",
    "    - Feedback: Quality assessment based on logical structure\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"question\"],\n",
    "        \"response\": f\"Let me solve this step by step:\\n{example['rationale']}\\n\\nTherefore, {example['answer']}\",\n",
    "        \"feedback\": \"Good reasoning with clear logical steps\" if len(example[\"rationale\"].split()) > 20 else \"Needs more detailed explanation\"\n",
    "    }\n",
    "\n",
    "def process_openthoughts(example):\n",
    "    \"\"\"Process OpenThoughts dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Open-ended prompt\n",
    "    - Response: Thought process and conclusion\n",
    "    - Feedback: Based on reasoning depth\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"prompt\"],\n",
    "        \"response\": f\"Let me think through this:\\n{example['thought_process']}\\n\\nConclusion: {example['response']}\",\n",
    "        \"feedback\": example.get(\"feedback\", \"Clear thought process with logical progression\")\n",
    "    }\n",
    "\n",
    "def process_skunkworks(example):\n",
    "    \"\"\"Process SkunkworksAI reasoning dataset.\n",
    "    \n",
    "    Format:\n",
    "    - Input: Reasoning task\n",
    "    - Response: Structured solution\n",
    "    - Feedback: Based on step-by-step approach\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"instruction\": example[\"instruction\"],\n",
    "        \"response\": example[\"output\"],\n",
    "        \"feedback\": \"Excellent step-by-step reasoning\" if \"step\" in example[\"output\"].lower() else \"Could use more explicit steps\"\n",
    "    }\n",
    "\n",
    "print(\"\\nProcessing datasets...\")\n",
    "\n",
    "# Process datasets with progress tracking\n",
    "processed_datasets = {}\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"Processing {name}...\")\n",
    "    if name == \"natural_reasoning\":\n",
    "        processed_datasets[name] = dataset.map(process_natural_reasoning)\n",
    "    elif name == \"openthoughts\":\n",
    "        processed_datasets[name] = dataset.map(process_openthoughts)\n",
    "    else:\n",
    "        processed_datasets[name] = dataset.map(process_skunkworks)\n",
    "\n",
    "# Sample and combine datasets with balanced representation\n",
    "sample_sizes = {\n",
    "    \"natural_reasoning\": 50000,\n",
    "    \"openthoughts\": 30000,\n",
    "    \"skunkworks\": 20000\n",
    "}\n",
    "\n",
    "combined_dataset = concatenate_datasets([\n",
    "    processed_datasets[name].select(range(min(size, len(processed_datasets[name]))))\n",
    "    for name, size in sample_sizes.items()\n",
    "])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "combined_dataset = combined_dataset.shuffle(seed=42)\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format example for Unsloth training.\n",
    "    \n",
    "    Based on Unsloth's documentation and examples.\n",
    "    \"\"\"\n",
    "    return f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['response']}\\n\\n### Feedback:\\n{example['feedback']}\"\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample formatted prompt:\")\n",
    "print(\"-\" * 80)\n",
    "print(format_prompt(combined_dataset[0]))\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(combined_dataset):,} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure GRPO Training\n",
    "\n",
    "Using Unsloth's GRPO implementation with optimized settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from unsloth.grpo import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Configure GRPO parameters\n",
    "grpo_config = GRPOConfig(\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    output_dir=\"./grpo_results\"\n",
    ")\n",
    "\n",
    "# Initialize GRPO trainer\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    args=grpo_config,\n",
    "    train_dataset=combined_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Reward Function\n",
    "\n",
    "Based on Unsloth's reward model design and theLMbook's evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_reasoning_reward(response, reference):\n",
    "    \"\"\"Evaluate reasoning quality using Unsloth's reward model approach.\n",
    "    \n",
    "    Based on:\n",
    "    - Unsloth's GRPO documentation\n",
    "    - theLMbook's evaluation metrics\n",
    "    - DeepSeek-R1 paper\n",
    "    \"\"\"\n",
    "    reward = 0.0\n",
    "    \n",
    "    # Step-by-step reasoning (0.3)\n",
    "    if \"step by step\" in response.lower():\n",
    "        reward += 0.2\n",
    "    if any(str(i) for i in range(1, 10) if f\"{i}.\" in response):\n",
    "        reward += 0.1\n",
    "    \n",
    "    # Explanatory depth (0.2)\n",
    "    depth_indicators = [\"because\", \"therefore\", \"as a result\", \"this means\"]\n",
    "    reward += 0.1 * sum(1 for indicator in depth_indicators if indicator in response.lower())\n",
    "    \n",
    "    # Matching key concepts with reference (0.3)\n",
    "    key_concepts = set(reference.lower().split()) & set(response.lower().split())\n",
    "    reward += min(0.3, len(key_concepts) * 0.02)\n",
    "    \n",
    "    return torch.tensor(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Start training\n",
    "trainer.train(\n",
    "    resume_from_checkpoint=False,\n",
    "    reward_function=compute_reasoning_reward\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We'll evaluate the model using Unsloth's evaluation framework and standard reasoning benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test cases for different reasoning types\n",
    "test_cases = {\n",
    "    \"scientific\": [\n",
    "        \"Explain how photosynthesis works in plants, breaking down the process into steps.\",\n",
    "        \"Why do objects fall towards Earth? Explain using physics principles.\"\n",
    "    ],\n",
    "    \"mathematical\": [\n",
    "        \"Solve the equation: 2x + 5 = 15. Show your work step by step.\",\n",
    "        \"Calculate the area of a triangle with base 6 and height 8. Explain your reasoning.\"\n",
    "    ],\n",
    "    \"logical\": [\n",
    "        \"All birds have feathers. A penguin is a bird. What can we conclude about penguins?\",\n",
    "        \"If it's raining, the streets are wet. The streets are wet. Can we conclude it's raining?\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "print(\"=== Model Evaluation Results ===\\n\")\n",
    "\n",
    "all_metrics = []\n",
    "for category, prompts in test_cases.items():\n",
    "    print(f\"\\n{category.upper()} REASONING TASKS:\\n\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Evaluate response\n",
    "        score = compute_reasoning_reward(response, prompt)\n",
    "        \n",
    "        print(f\"\\nResponse:\\n{response}\")\n",
    "        print(f\"\\nScore: {score:.2f}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Save the model\n",
    "output_dir = \"unsloth-grpo-finetuned\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"\\nModel saved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}