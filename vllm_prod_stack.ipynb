{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Production Stack with KV Cache Offloading on Brev.dev\n",
    "\n",
    "This notebook guides you through setting up a vLLM production stack on brev.dev with a focus on KV Cache offloading. We'll cover:\n",
    "\n",
    "1. Setting up a Kubernetes environment with MicroK8s\n",
    "2. Installing the vLLM production stack using Helm\n",
    "3. Configuring KV Cache offloading to CPU\n",
    "4. Setting up remote shared KV Cache storage\n",
    "5. Testing and benchmarking the setup\n",
    "\n",
    "## What is vLLM?\n",
    "\n",
    "vLLM is a high-performance library for LLM inference and serving. It's designed to maximize throughput and minimize latency for LLM applications. Key features include:\n",
    "\n",
    "- PagedAttention for efficient memory management\n",
    "- Continuous batching to handle concurrent requests\n",
    "- Optimized CUDA kernels for faster execution\n",
    "- OpenAI-compatible API for easy integration\n",
    "\n",
    "## What is KV Cache?\n",
    "\n",
    "The KV (Key-Value) cache is a critical component in transformer-based language models:\n",
    "\n",
    "- During generation, LLMs compute attention over previously generated tokens\n",
    "- The KV cache stores the key and value tensors from previous tokens to avoid recomputation\n",
    "- This significantly speeds up inference but consumes a lot of GPU memory\n",
    "- As conversations get longer, the KV cache grows linearly with the sequence length\n",
    "\n",
    "```\n",
    "┌───────────────────┐\n",
    "│     LLM Model     │\n",
    "└─────────┬─────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌───────────────────┐\n",
    "│    KV Cache in    │\n",
    "│    GPU Memory     │◄── Memory bottleneck for long sequences\n",
    "└───────────────────┘\n",
    "```\n",
    "\n",
    "## Why Offload KV Cache?\n",
    "\n",
    "KV cache offloading addresses several challenges:\n",
    "\n",
    "- **Memory Efficiency**: Frees up valuable GPU memory for model parameters\n",
    "- **Longer Contexts**: Enables processing of much longer conversations\n",
    "- **Cost Optimization**: Allows using smaller/fewer GPUs for the same workload\n",
    "- **Higher Throughput**: Serves more concurrent users with the same hardware\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Prerequisites\n",
    "\n",
    "### What is Kubernetes and why use it for LLM deployment?\n",
    "\n",
    "Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. For LLM deployments, Kubernetes offers several advantages:\n",
    "\n",
    "- **Scalability**: Easily scale your LLM services up or down based on demand\n",
    "- **Resource Management**: Efficiently allocate GPU and CPU resources across workloads\n",
    "- **High Availability**: Ensure your LLM services remain available even if individual components fail\n",
    "- **Declarative Configuration**: Define your entire LLM infrastructure as code\n",
    "\n",
    "We'll use MicroK8s, a lightweight Kubernetes distribution that's easy to set up and use on a single machine.\n",
    "\n",
    "First, let's check if we have GPU support available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Required Tools\n",
    "\n",
    "Let's create a script to install MicroK8s and set up our Kubernetes environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup_microk8s.sh\n",
    "#!/bin/bash\n",
    "# Function to check command status\n",
    "check_status() {\n",
    "    if [ $? -ne 0 ]; then\n",
    "        echo \"Error: $1 failed\"\n",
    "        exit 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Function to check if command exists\n",
    "command_exists() {\n",
    "    command -v \"$1\" >/dev/null 2>&1\n",
    "}\n",
    "\n",
    "# Check if nvidia-smi is available\n",
    "echo \"Checking NVIDIA GPU...\"\n",
    "if ! command_exists nvidia-smi; then\n",
    "    echo \"Warning: nvidia-smi not found. GPU support may not be available.\"\n",
    "else\n",
    "    nvidia-smi\n",
    "    check_status \"nvidia-smi\"\n",
    "fi\n",
    "\n",
    "# Install MicroK8s\n",
    "echo \"Installing MicroK8s...\"\n",
    "sudo snap install microk8s --classic --channel=1.25/stable\n",
    "check_status \"MicroK8s installation\"\n",
    "\n",
    "# Add user to microk8s group\n",
    "echo \"Adding user to microk8s group...\"\n",
    "sudo usermod -a -G microk8s $USER\n",
    "check_status \"Adding user to microk8s group\"\n",
    "\n",
    "# Create and set permissions for .kube directory\n",
    "echo \"Setting up .kube directory...\"\n",
    "mkdir -p ~/.kube\n",
    "chmod 0700 ~/.kube\n",
    "sudo chown -f -R $USER ~/.kube\n",
    "check_status \"Setting up .kube directory\"\n",
    "\n",
    "# Wait for MicroK8s to be ready\n",
    "echo \"Waiting for MicroK8s to be ready...\"\n",
    "sudo microk8s status --wait-ready\n",
    "check_status \"MicroK8s ready check\"\n",
    "\n",
    "# Enable GPU and storage support\n",
    "echo \"Enabling GPU and hostpath-storage...\"\n",
    "sudo microk8s enable gpu hostpath-storage\n",
    "check_status \"Enabling MicroK8s addons\"\n",
    "\n",
    "# Double check status\n",
    "echo \"Checking final MicroK8s status...\"\n",
    "sudo microk8s status --wait-ready\n",
    "check_status \"Final MicroK8s status check\"\n",
    "\n",
    "# Set up Helm repositories\n",
    "echo \"Setting up Helm repositories...\"\n",
    "sudo microk8s helm repo remove nvidia || true  # Remove if exists\n",
    "sudo microk8s helm repo add nvidia https://helm.ngc.nvidia.com/nvidia\n",
    "sudo microk8s helm repo update\n",
    "\n",
    "# Activate the new group membership without requiring logout\n",
    "echo \"Activating microk8s group membership...\"\n",
    "if ! groups | grep -q microk8s; then\n",
    "    exec sg microk8s -c '\n",
    "        echo \"Testing cluster access...\"\n",
    "        sudo microk8s kubectl get services\n",
    "        sudo microk8s kubectl get nodes\n",
    "        echo \"Creating example nginx deployment...\"\n",
    "        sudo microk8s kubectl create deployment nginx --image=nginx\n",
    "        echo \"Checking pods...\"\n",
    "        sudo microk8s kubectl get pods\n",
    "        echo \"The kubectl and helm aliases are now active globally.\"\n",
    "    '\n",
    "else\n",
    "    echo \"Testing cluster access...\"\n",
    "    sudo microk8s kubectl get services\n",
    "    sudo microk8s kubectl get nodes\n",
    "    echo \"Creating example nginx deployment...\"\n",
    "    sudo microk8s kubectl create deployment nginx --image=nginx\n",
    "    echo \"Checking pods...\"\n",
    "    sudo microk8s kubectl get pods\n",
    "fi\n",
    "\n",
    "echo \"Setup of MicroK8s is complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the script executable and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x setup_microk8s.sh\n",
    "!./setup_microk8s.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Clone the vLLM Production Stack Repository\n",
    "\n",
    "Let's clone the vLLM production stack repository to get access to the necessary configuration files and examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/vllm-project/production-stack.git\n",
    "!cd production-stack && ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minimal vLLM Installation\n",
    "\n",
    "### Understanding Helm and vLLM Stack\n",
    "\n",
    "Helm is a package manager for Kubernetes that simplifies the deployment and management of applications. Think of it as npm for Node.js or pip for Python, but for Kubernetes applications.\n",
    "\n",
    "The vLLM production stack consists of several components:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│                 Client Request                  │\n",
    "└───────────────────────┬─────────────────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│                Router Service                   │\n",
    "│  (Load balancing and request distribution)      │\n",
    "└───────────────────────┬─────────────────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│               vLLM Engine Pods                  │\n",
    "│  (Model serving with GPU acceleration)          │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Let's start with a minimal installation of vLLM to ensure everything is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the vLLM Helm repository\n",
    "!sudo microk8s helm repo add vllm https://vllm-project.github.io/production-stack\n",
    "!sudo microk8s helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a minimal configuration file for our initial deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile minimal-vllm-config.yaml\n",
    "servingEngineSpec:\n",
    "  runtimeClassName: \"\"                  # Runtime class name (leave empty for default)\n",
    "  modelSpec:\n",
    "  - name: \"opt125m\"                     # Name for the deployment\n",
    "    repository: \"vllm/vllm-openai\"      # Docker image for vLLM\n",
    "    tag: \"latest\"                       # Image tag\n",
    "    modelURL: \"facebook/opt-125m\"       # Small model for testing\n",
    "    replicaCount: 1                     # Single replica\n",
    "    requestCPU: 6                       # CPU cores requested\n",
    "    requestMemory: \"16Gi\"               # Memory requested\n",
    "    requestGPU: 1                       # Number of GPUs requested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deploy the minimal vLLM stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f minimal-vllm-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods\n",
    "!sudo microk8s kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our deployment by forwarding the service port and sending a request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 30080:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API by listing available models\n",
    "!curl -o- http://localhost:30080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the completion endpoint\n",
    "!curl -X POST http://localhost:30080/v1/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"facebook/opt-125m\",\n",
    "    \"prompt\": \"Once upon a time,\",\n",
    "    \"max_tokens\": 10\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuring KV Cache Offloading to CPU\n",
    "\n",
    "### How CPU Offloading Works\n",
    "\n",
    "KV cache offloading to CPU works by moving the key-value tensors from GPU memory to CPU memory when they're not immediately needed, then bringing them back when required for inference.\n",
    "\n",
    "```\n",
    "┌───────────────────┐\n",
    "│     LLM Model     │\n",
    "└─────────┬─────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌───────────────────┐         ┌───────────────────┐\n",
    "│  Active KV Cache  │◄───────►│ Offloaded KV Cache│\n",
    "│   (GPU Memory)    │         │   (CPU Memory)    │\n",
    "└───────────────────┘         └───────────────────┘\n",
    "```\n",
    "\n",
    "**Benefits of CPU Offloading:**\n",
    "- Allows handling longer sequences than would fit in GPU memory alone\n",
    "- Enables more concurrent requests with the same GPU resources\n",
    "- Provides a good balance between performance and memory efficiency\n",
    "\n",
    "We'll use LMCache, which integrates with vLLM to provide efficient KV cache offloading capabilities.\n",
    "\n",
    "Now that we have a basic vLLM deployment working, let's configure KV cache offloading to CPU using LMCache. First, let's uninstall our previous deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm uninstall vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a configuration file for KV cache offloading to CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpu-offloading-config.yaml\n",
    "servingEngineSpec:\n",
    "  modelSpec:\n",
    "  - name: \"mistral\"                      # Name for the deployment\n",
    "    repository: \"lmcache/vllm-openai\"    # Docker image with LMCache support\n",
    "    tag: \"latest\"                        # Image tag\n",
    "    modelURL: \"mistralai/Mistral-7B-Instruct-v0.2\"  # HuggingFace model ID\n",
    "    replicaCount: 1                      # Number of replicas to deploy\n",
    "    requestCPU: 10                       # CPU cores requested\n",
    "    requestMemory: \"40Gi\"                # Memory requested\n",
    "    requestGPU: 1                        # Number of GPUs requested\n",
    "    pvcStorage: \"50Gi\"                   # Persistent volume size\n",
    "    vllmConfig:                          # vLLM-specific configuration\n",
    "      enableChunkedPrefill: false        # Disable chunked prefill\n",
    "      enablePrefixCaching: false         # Disable prefix caching\n",
    "      maxModelLen: 16384                 # Maximum sequence length\n",
    "    \n",
    "    lmcacheConfig:                       # LMCache configuration\n",
    "      enabled: true                      # Enable LMCache\n",
    "      cpuOffloadingBufferSize: \"20\"      # 20GB of CPU memory for KV cache\n",
    "    \n",
    "    hf_token: \"\"                         # HuggingFace token (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deploy the vLLM stack with KV cache offloading to CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f cpu-offloading-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the logs to verify that LMCache is active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod name for the vLLM deployment\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}') && \\\n",
    "sudo microk8s kubectl logs $POD_NAME | grep -i lmcache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our deployment with KV cache offloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 30080:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API by listing available models\n",
    "!curl -o- http://localhost:30080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the completion endpoint\n",
    "!curl -X POST http://localhost:30080/v1/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"prompt\": \"Explain the significance of KV cache in language models.\",\n",
    "    \"max_tokens\": 100\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting Up Remote Shared KV Cache Storage\n",
    "\n",
    "### Understanding Remote Shared KV Cache\n",
    "\n",
    "Remote shared KV cache takes the concept of offloading a step further by allowing multiple vLLM instances to share a common KV cache storage. This approach has several advantages:\n",
    "\n",
    "```\n",
    "┌───────────────┐     ┌───────────────┐     ┌───────────────┐\n",
    "│  vLLM Pod #1  │     │  vLLM Pod #2  │     │  vLLM Pod #3  │\n",
    "└───────┬───────┘     └───────┬───────┘     └───────┬───────┘\n",
    "        │                     │                     │\n",
    "        └─────────────┬───────┴─────────────┬───────┘\n",
    "                      │                     │\n",
    "                      ▼                     ▼\n",
    "        ┌───────────────────────┐ ┌───────────────────────┐\n",
    "        │   Local KV Cache      │ │   Shared KV Cache     │\n",
    "        │   (GPU Memory)        │ │   (Remote Storage)    │\n",
    "        └───────────────────────┘ └───────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Resource Efficiency**: Multiple model instances can share cached KV pairs\n",
    "- **Fault Tolerance**: If one pod fails, others can still access the shared cache\n",
    "- **Horizontal Scaling**: Add more vLLM pods without duplicating cached data\n",
    "- **Consistent Performance**: Provides more predictable performance across pods\n",
    "\n",
    "Now let's set up remote shared KV cache storage. First, let's uninstall our previous deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm uninstall vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a configuration file for remote shared KV cache storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile remote-shared-storage-config.yaml\n",
    "servingEngineSpec:\n",
    "  runtimeClassName: \"\"\n",
    "  modelSpec:\n",
    "  - name: \"mistral\"                      # Name for the deployment\n",
    "    repository: \"lmcache/vllm-openai\"    # Docker image with LMCache support\n",
    "    tag: \"latest\"                        # Image tag\n",
    "    modelURL: \"mistralai/Mistral-7B-Instruct-v0.2\"  # HuggingFace model ID\n",
    "    replicaCount: 2                      # Deploy 2 replicas to demonstrate sharing\n",
    "    requestCPU: 10                       # CPU cores requested\n",
    "    requestMemory: \"40Gi\"                # Memory requested\n",
    "    requestGPU: 1                        # Number of GPUs requested\n",
    "    pvcStorage: \"50Gi\"                   # Persistent volume size\n",
    "    vllmConfig:                          # vLLM-specific configuration\n",
    "      enableChunkedPrefill: false        # Disable chunked prefill\n",
    "      enablePrefixCaching: false         # Disable prefix caching\n",
    "      maxModelLen: 16384                 # Maximum sequence length\n",
    "    \n",
    "    lmcacheConfig:                       # LMCache configuration\n",
    "      enabled: true                      # Enable LMCache\n",
    "      cpuOffloadingBufferSize: \"20\"      # 20GB of CPU memory for KV cache\n",
    "    \n",
    "    hf_token: \"\"                         # HuggingFace token (if needed)\n",
    "\n",
    "# This section configures the shared cache server\n",
    "cacheserverSpec:\n",
    "  replicaCount: 1                        # Number of cache server replicas\n",
    "  containerPort: 8080                    # Container port\n",
    "  servicePort: 81                        # Service port\n",
    "  serde: \"naive\"                         # Serialization/deserialization method\n",
    "  repository: \"lmcache/vllm-openai\"      # Docker image\n",
    "  tag: \"latest\"                          # Image tag\n",
    "  resources:                             # Resource requests and limits\n",
    "    requests:\n",
    "      cpu: \"4\"                           # CPU cores requested\n",
    "      memory: \"8G\"                       # Memory requested\n",
    "    limits:\n",
    "      cpu: \"4\"                           # CPU cores limit\n",
    "      memory: \"10G\"                      # Memory limit\n",
    "  labels:                                # Kubernetes labels\n",
    "    environment: \"cacheserver\"           # Environment label\n",
    "    release: \"cacheserver\"               # Release label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deploy the vLLM stack with remote shared KV cache storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f remote-shared-storage-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the logs to verify that LMCache with remote shared storage is active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod name for the vLLM deployment\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}' | head -1) && \\\n",
    "sudo microk8s kubectl logs $POD_NAME | grep -i lmcache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our deployment with remote shared KV cache storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 30080:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the completion endpoint\n",
    "!curl -X POST http://localhost:30080/v1/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"prompt\": \"Explain the significance of KV cache in language models.\",\n",
    "    \"max_tokens\": 100\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmarking KV Cache Offloading\n",
    "\n",
    "### Why Benchmark KV Cache Offloading?\n",
    "\n",
    "Benchmarking helps us understand the performance characteristics and trade-offs of KV cache offloading:\n",
    "\n",
    "- **First Request Latency**: Initial requests might be slower due to cache setup\n",
    "- **Subsequent Request Performance**: Later requests should benefit from cached KV pairs\n",
    "- **Memory Usage Patterns**: How memory is distributed between GPU and CPU/remote storage\n",
    "- **Throughput Under Load**: How the system performs with concurrent requests\n",
    "\n",
    "Our benchmark will focus on measuring request latency across multiple identical requests, which should demonstrate the benefits of KV cache reuse.\n",
    "\n",
    "```\n",
    "Request 1: [■■■■■■■■■■] 100% (Initial - No cache benefit)\n",
    "Request 2: [■■■■■■■■  ] 80%  (Some cache hits)\n",
    "Request 3: [■■■■■■    ] 60%  (More cache hits)\n",
    "Request 4: [■■■■      ] 40%  (Significant cache hits)\n",
    "Request 5: [■■■       ] 30%  (Maximum cache benefit)\n",
    "```\n",
    "\n",
    "Let's create a simple benchmark to test the performance of KV cache offloading. We'll send multiple requests with the same prompt to see how the KV cache offloading affects performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile benchmark.py\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def send_request(prompt, max_tokens=100):\n",
    "    \"\"\"Send a completion request to the vLLM API and measure response time\"\"\"\n",
    "    url = \"http://localhost:30080/v1/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return {\n",
    "        \"status_code\": response.status_code,\n",
    "        \"response\": response.json() if response.status_code == 200 else None,\n",
    "        \"time\": end_time - start_time\n",
    "    }\n",
    "\n",
    "def run_benchmark(prompt, num_requests=5):\n",
    "    \"\"\"Run a benchmark with multiple identical requests to measure KV cache benefits\"\"\"\n",
    "    print(f\"Running benchmark with {num_requests} requests...\")\n",
    "    times = []\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        print(f\"Request {i+1}/{num_requests}\")\n",
    "        result = send_request(prompt)\n",
    "        if result[\"status_code\"] == 200:\n",
    "            times.append(result[\"time\"])\n",
    "            print(f\"Request {i+1} completed in {result['time']:.2f} seconds\")\n",
    "        else:\n",
    "            print(f\"Request {i+1} failed with status code {result['status_code']}\")\n",
    "    \n",
    "    if times:\n",
    "        print(\"\\nBenchmark Results:\")\n",
    "        print(f\"Average time: {statistics.mean(times):.2f} seconds\")\n",
    "        print(f\"Median time: {statistics.median(times):.2f} seconds\")\n",
    "        print(f\"Min time: {min(times):.2f} seconds\")\n",
    "        print(f\"Max time: {max(times):.2f} seconds\")\n",
    "        if len(times) > 1:\n",
    "            print(f\"Standard deviation: {statistics.stdev(times):.2f} seconds\")\n",
    "        \n",
    "        # Create a visualization of the results\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(1, len(times) + 1), times, marker='o', linestyle='-', color='blue')\n",
    "            plt.title('Request Latency Over Time (KV Cache Effect)')\n",
    "            plt.xlabel('Request Number')\n",
    "            plt.ylabel('Time (seconds)')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.savefig('kv_cache_benchmark.png')\n",
    "            print(\"\\nBenchmark visualization saved to 'kv_cache_benchmark.png'\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nCouldn't create visualization: {e}\")\n",
    "    else:\n",
    "        print(\"No successful requests to analyze.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Using a consistent prompt to demonstrate KV cache benefits\n",
    "    prompt = \"Explain the significance of KV cache in language models. Provide details about how it improves inference performance and what are the trade-offs involved.\"\n",
    "    run_benchmark(prompt, num_requests=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python benchmark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "Let's clean up our deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm uninstall vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. Set up a Kubernetes environment with MicroK8s\n",
    "2. Installed the vLLM production stack using Helm\n",
    "3. Configured KV Cache offloading to CPU using LMCache\n",
    "4. Set up remote shared KV Cache storage\n",
    "5. Tested and benchmarked the setup\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Memory Efficiency**: KV cache offloading allows you to serve larger models and longer sequences with the same GPU resources\n",
    "- **Scalability**: Remote shared KV cache enables efficient horizontal scaling of your LLM deployment\n",
    "- **Performance Trade-offs**: There's a balance between keeping KV cache in GPU memory (faster) vs. offloading (more memory efficient)\n",
    "- **Production Readiness**: The vLLM production stack with LMCache provides a robust, Kubernetes-native solution for deploying LLMs at scale\n",
    "\n",
    "### When to Use KV Cache Offloading\n",
    "\n",
    "Consider implementing KV cache offloading when:\n",
    "- You need to support long conversations or documents\n",
    "- You want to maximize the number of concurrent users per GPU\n",
    "- You're working with limited GPU memory but have ample CPU memory\n",
    "- You need to scale horizontally while maintaining efficient resource usage\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To further enhance your vLLM deployment, consider exploring:\n",
    "- Fine-tuning the offloading parameters based on your specific workload\n",
    "- Implementing monitoring and observability for your deployment\n",
    "- Setting up auto-scaling based on request load\n",
    "- Exploring other vLLM features like LoRA adapters for model customization\n",
    "\n",
    "The vLLM production stack with LMCache provides a robust solution for deploying LLMs in production with KV Cache offloading capabilities, enabling you to build more efficient and scalable AI applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
