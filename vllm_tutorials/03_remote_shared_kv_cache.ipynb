{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remote Shared KV Cache with vLLM Production Stack\n",
    "\n",
    "This notebook demonstrates how to set up remote shared KV cache storage in the vLLM production stack, allowing multiple vLLM instances to share a common KV cache for improved efficiency and fault tolerance.\n",
    "\n",
    "## Understanding Remote Shared KV Cache\n",
    "\n",
    "Remote shared KV cache takes the concept of offloading a step further by allowing multiple vLLM instances to share a common KV cache storage. This approach has several advantages:\n",
    "\n",
    "```\n",
    "┌───────────────┐     ┌───────────────┐     ┌───────────────┐\n",
    "│  vLLM Pod #1  │     │  vLLM Pod #2  │     │  vLLM Pod #3  │\n",
    "└───────┬───────┘     └───────┬───────┘     └───────┬───────┘\n",
    "        │                     │                     │\n",
    "        └─────────────┬───────┴─────────────┬───────┘\n",
    "                      │                     │\n",
    "                      ▼                     ▼\n",
    "        ┌───────────────────────┐ ┌───────────────────────┐\n",
    "        │   Local KV Cache      │ │   Shared KV Cache     │\n",
    "        │   (GPU Memory)        │ │   (Remote Storage)    │\n",
    "        └───────────────────────┘ └───────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Resource Efficiency**: Multiple model instances can share cached KV pairs\n",
    "- **Fault Tolerance**: If one pod fails, others can still access the shared cache\n",
    "- **Horizontal Scaling**: Add more vLLM pods without duplicating cached data\n",
    "- **Consistent Performance**: Provides more predictable performance across pods\n",
    "\n",
    "## Recommended GPU VM Configuration\n",
    "\n",
    "For this tutorial, we recommend:\n",
    "\n",
    "- **GPU**: 2x NVIDIA A10G or better (24GB+ VRAM each)\n",
    "- **CPU**: 16+ cores\n",
    "- **RAM**: 64GB+\n",
    "- **Storage**: 100GB+ SSD\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before proceeding, make sure you have completed the setup in the first notebook (`01_setup_vllm_production_stack.ipynb`). Let's verify that our Kubernetes environment is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How Remote Shared KV Cache Works\n",
    "\n",
    "Remote shared KV cache works by setting up a dedicated cache server that multiple vLLM instances can connect to. When a vLLM instance generates tokens, it stores the KV cache entries in the shared cache server, and other instances can retrieve and reuse these entries.\n",
    "\n",
    "This is particularly useful for:\n",
    "- **Stateless scaling**: Instances can be added or removed without losing cached data\n",
    "- **Load balancing**: Requests can be distributed across instances while maintaining cache benefits\n",
    "- **High availability**: If one instance fails, others can continue serving requests using the shared cache\n",
    "\n",
    "In the vLLM production stack, this is implemented using LMCache's remote storage capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring Remote Shared KV Cache\n",
    "\n",
    "Let's create a configuration file for remote shared KV cache. We'll deploy two vLLM instances that share a common KV cache server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile remote-shared-storage-config.yaml\n",
    "servingEngineSpec:\n",
    "  runtimeClassName: \"\"\n",
    "  modelSpec:\n",
    "  - name: \"mistral\"                      # Name for the deployment\n",
    "    repository: \"lmcache/vllm-openai\"    # Docker image with LMCache support\n",
    "    tag: \"latest\"                        # Image tag\n",
    "    modelURL: \"mistralai/Mistral-7B-Instruct-v0.3\"  # HuggingFace model ID\n",
    "    replicaCount: 2                      # Deploy 2 replicas to demonstrate sharing\n",
    "    requestCPU: 10                       # CPU cores requested\n",
    "    requestMemory: \"40Gi\"                # Memory requested\n",
    "    requestGPU: 1                        # Number of GPUs requested\n",
    "    pvcStorage: \"50Gi\"                   # Persistent volume size\n",
    "    vllmConfig:                          # vLLM-specific configuration\n",
    "      enableChunkedPrefill: false        # Disable chunked prefill\n",
    "      enablePrefixCaching: false         # Disable prefix caching\n",
    "      maxModelLen: 16384                 # Maximum sequence length\n",
    "    \n",
    "    lmcacheConfig:                       # LMCache configuration\n",
    "      enabled: true                      # Enable LMCache\n",
    "      cpuOffloadingBufferSize: \"20\"      # 20GB of CPU memory for KV cache\n",
    "    \n",
    "    hf_token: \"\"                         # HuggingFace token (if needed)\n",
    "\n",
    "# This section configures the shared cache server\n",
    "cacheserverSpec:\n",
    "  replicaCount: 1                        # Number of cache server replicas\n",
    "  containerPort: 8080                    # Container port\n",
    "  servicePort: 81                        # Service port\n",
    "  serde: \"naive\"                         # Serialization/deserialization method\n",
    "  repository: \"lmcache/vllm-openai\"      # Docker image\n",
    "  tag: \"latest\"                          # Image tag\n",
    "  resources:                             # Resource requests and limits\n",
    "    requests:\n",
    "      cpu: \"4\"                           # CPU cores requested\n",
    "      memory: \"8G\"                       # Memory requested\n",
    "    limits:\n",
    "      cpu: \"4\"                           # CPU cores limit\n",
    "      memory: \"10G\"                      # Memory limit\n",
    "  labels:                                # Kubernetes labels\n",
    "    environment: \"cacheserver\"           # Environment label\n",
    "    release: \"cacheserver\"               # Release label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Configuration Parameters Explained\n",
    "\n",
    "- **replicaCount: 2**: We're deploying two vLLM instances to demonstrate shared caching\n",
    "- **cacheserverSpec**: This section configures the dedicated cache server\n",
    "  - **replicaCount: 1**: We're deploying one cache server instance\n",
    "  - **serde: \"naive\"**: The serialization method for cache entries\n",
    "  - **resources**: CPU and memory resources for the cache server\n",
    "\n",
    "Now let's deploy the vLLM stack with remote shared KV cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f remote-shared-storage-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our deployment. This might take a few minutes as the model is downloaded and loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the logs to verify that LMCache with remote shared storage is active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod name for the first vLLM deployment\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}' | head -1) && \\\n",
    "sudo microk8s kubectl logs $POD_NAME | grep -i lmcache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the logs of the cache server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod name for the cache server\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep cacheserver | awk '{print $1}') && \\\n",
    "sudo microk8s kubectl logs $POD_NAME | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Remote Shared KV Cache\n",
    "\n",
    "Let's test our deployment with remote shared KV cache by sending requests to both vLLM instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 30080:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API by listing available models\n",
    "!curl -o- http://localhost:30080/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a script to send multiple requests and observe the shared cache behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_shared_cache.py\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import threading\n",
    "\n",
    "def send_request(prompt, session_id, max_tokens=100):\n",
    "    \"\"\"Send a completion request to the vLLM API\"\"\"\n",
    "    url = \"http://localhost:30080/v1/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        # Use a consistent session ID to simulate the same user across different pods\n",
    "        \"user\": f\"user-{session_id}\"\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return {\n",
    "        \"status_code\": response.status_code,\n",
    "        \"response\": response.json() if response.status_code == 200 else None,\n",
    "        \"time\": end_time - start_time\n",
    "    }\n",
    "\n",
    "def run_session(session_id, prompt, num_requests=3):\n",
    "    \"\"\"Run a series of requests with the same session ID\"\"\"\n",
    "    print(f\"Starting session {session_id}\")\n",
    "    times = []\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        print(f\"Session {session_id}, Request {i+1}/{num_requests}\")\n",
    "        result = send_request(prompt, session_id)\n",
    "        if result[\"status_code\"] == 200:\n",
    "            times.append(result[\"time\"])\n",
    "            print(f\"Session {session_id}, Request {i+1} completed in {result['time']:.2f} seconds\")\n",
    "            # Add a small delay to simulate real-world usage\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "        else:\n",
    "            print(f\"Session {session_id}, Request {i+1} failed with status code {result['status_code']}\")\n",
    "    \n",
    "    if times:\n",
    "        print(f\"\\nSession {session_id} Results:\")\n",
    "        print(f\"Average time: {sum(times)/len(times):.2f} seconds\")\n",
    "        print(f\"First request: {times[0]:.2f} seconds\")\n",
    "        print(f\"Last request: {times[-1]:.2f} seconds\")\n",
    "        print(f\"Improvement: {(times[0] - times[-1])/times[0]*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"No successful requests for session {session_id}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a prompt that will benefit from KV cache\n",
    "    prompt = \"\"\"Explain the concept of distributed systems and how they relate to cloud computing. \n",
    "    Include information about consistency models, fault tolerance, and scalability.\"\"\"\n",
    "    \n",
    "    # Run multiple sessions in parallel to simulate different users\n",
    "    threads = []\n",
    "    for i in range(3):\n",
    "        thread = threading.Thread(target=run_session, args=(i, prompt))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    \n",
    "    # Wait for all sessions to complete\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the test script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test_shared_cache.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the logs of both vLLM instances and the cache server to see the shared cache in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod names for the vLLM deployments\n",
    "!POD_NAMES=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}') && \\\n",
    "for POD in $POD_NAMES; do \\\n",
    "    echo \"\\nLogs for $POD:\" && \\\n",
    "    sudo microk8s kubectl logs $POD | grep -i \"cache\\|hit\\|miss\" | tail -10; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the cache server logs\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep cacheserver | awk '{print $1}') && \\\n",
    "sudo microk8s kubectl logs $POD_NAME | grep -i \"cache\\|hit\\|miss\" | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demonstrating Fault Tolerance\n",
    "\n",
    "One of the key benefits of remote shared KV cache is fault tolerance. Let's demonstrate this by deleting one of the vLLM pods and observing how the system continues to function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod names for the vLLM deployments\n",
    "!POD_NAMES=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}') && \\\n",
    "POD_TO_DELETE=$(echo $POD_NAMES | awk '{print $1}') && \\\n",
    "echo \"Deleting pod $POD_TO_DELETE\" && \\\n",
    "sudo microk8s kubectl delete pod $POD_TO_DELETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait for Kubernetes to create a new pod to replace the deleted one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the new pod to be created\n",
    "import time\n",
    "print(\"Waiting for new pod to be created...\")\n",
    "for i in range(10):\n",
    "    !sudo microk8s kubectl get pods | grep vllm-mistral-deployment\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run our test script again to see if the shared cache is still working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test_shared_cache.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleanup\n",
    "\n",
    "When you're done, you can clean up the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm uninstall vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. Configured remote shared KV cache using LMCache\n",
    "2. Deployed multiple vLLM instances that share a common KV cache server\n",
    "3. Tested the deployment with parallel requests\n",
    "4. Demonstrated the fault tolerance of the system\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Remote shared KV cache allows multiple vLLM instances to share cached data\n",
    "- This improves resource efficiency and enables horizontal scaling\n",
    "- The system is more fault-tolerant, as cached data persists even if individual pods fail\n",
    "- Performance is more consistent across instances\n",
    "\n",
    "### When to Use Remote Shared KV Cache\n",
    "\n",
    "Consider implementing remote shared KV cache when:\n",
    "- You need to deploy multiple vLLM instances for high availability\n",
    "- You want to scale horizontally while maintaining efficient resource usage\n",
    "- You need fault tolerance for your LLM deployment\n",
    "- You have many users sending similar queries\n",
    "\n",
    "Remote shared KV cache is a powerful technique for building scalable, efficient, and fault-tolerant LLM deployments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}