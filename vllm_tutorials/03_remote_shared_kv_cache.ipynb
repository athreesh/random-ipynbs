{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remote Shared KV Cache with vLLM and Mistral-7B-Instruct-v0.3\n",
    "\n",
    "This notebook demonstrates how to set up remote shared KV cache storage in vLLM, allowing multiple instances to share a common KV cache for improved efficiency and fault tolerance.\n",
    "\n",
    "## Understanding Remote Shared KV Cache\n",
    "\n",
    "Remote shared KV cache takes the concept of offloading a step further by allowing multiple vLLM instances to share a common KV cache storage:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                Remote Shared KV Cache                       │\n",
    "│                                                             │\n",
    "│  ┌─────────────┐     ┌─────────────┐     ┌─────────────┐   │\n",
    "│  │ vLLM Pod #1 │     │ vLLM Pod #2 │     │ vLLM Pod #3 │   │\n",
    "│  └─────┬───────┘     └─────┬───────┘     └─────┬───────┘   │\n",
    "│        │                   │                   │           │\n",
    "│        └─────────┬─────────┴─────────┬────────┘           │\n",
    "│                  │                   │                     │\n",
    "│                  ▼                   ▼                     │\n",
    "│      ┌───────────────────┐ ┌───────────────────┐          │\n",
    "│      │   Local KV Cache  │ │  Shared KV Cache  │          │\n",
    "│      │   (GPU Memory)    │ │  (Remote Storage) │          │\n",
    "│      └───────────────────┘ └───────────────────┘          │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "- Multiple model instances can share cached KV pairs\n",
    "- Fault tolerance: if one pod fails, others can still access the cache\n",
    "- Horizontal scaling: add more vLLM pods without duplicating cached data\n",
    "- Consistent performance across pods\n",
    "\n",
    "## Recommended GPU VM Configuration\n",
    "\n",
    "For remote shared KV cache with Mistral-7B-Instruct-v0.3, we recommend:\n",
    "\n",
    "- **GPU**: 2x NVIDIA A10G or better (24GB+ VRAM each)\n",
    "- **CPU**: 10+ cores\n",
    "- **RAM**: 40GB+\n",
    "- **Storage**: 100GB+ SSD\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up Remote Shared KV Cache\n",
    "\n",
    "First, let's create a configuration file that enables remote shared KV cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile shared-kv-config.yaml\n",
    "servingEngineSpec:\n",
    "  modelSpec:\n",
    "  - name: \"mistral\"                      # Name for the deployment\n",
    "    repository: \"lmcache/vllm-openai\"    # Docker image with LMCache support\n",
    "    tag: \"latest\"                        # Image tag\n",
    "    modelURL: \"mistralai/Mistral-7B-Instruct-v0.3\"  # HuggingFace model ID\n",
    "    replicaCount: 2                      # Number of replicas to deploy\n",
    "    requestCPU: 10                       # CPU cores requested\n",
    "    requestMemory: \"40Gi\"                # Memory requested\n",
    "    requestGPU: 1                        # Number of GPUs requested\n",
    "    pvcStorage: \"50Gi\"                   # Persistent volume size\n",
    "    vllmConfig:                          # vLLM-specific configuration\n",
    "      enableChunkedPrefill: false        # Disable chunked prefill\n",
    "      enablePrefixCaching: true          # Enable prefix caching\n",
    "      maxModelLen: 16384                 # Maximum sequence length\n",
    "    \n",
    "    lmcacheConfig:                       # LMCache configuration\n",
    "      enabled: true                      # Enable LMCache\n",
    "      cpuOffloadingBufferSize: \"20\"      # 20GB of CPU memory for KV cache\n",
    "      remoteSharedCache:                 # Remote shared cache configuration\n",
    "        enabled: true                    # Enable remote shared cache\n",
    "        endpoint: \"redis:6379\"           # Redis endpoint\n",
    "        maxCacheSize: \"50Gi\"             # Maximum shared cache size\n",
    "    \n",
    "    hf_token: \"\"                         # HuggingFace token (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Configuration\n",
    "\n",
    "Let's break down the key configuration parameters:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                Configuration Parameters                     │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Resource Config     │      │ Model Config            │   │\n",
    "│  │ - 10 CPU cores     │      │ - Mistral-7B-Instruct   │   │\n",
    "│  │ - 40GB RAM         │      │ - 16K context length     │   │\n",
    "│  │ - 1 GPU per pod    │      │ - Prefix caching on     │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ LMCache Config      │      │ Shared Cache Config     │   │\n",
    "│  │ - Enabled          │      │ - Redis backend         │   │\n",
    "│  │ - 20GB CPU buffer  │      │ - 50GB max size        │   │\n",
    "│  │ - Auto-offloading  │      │ - Shared across pods   │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Key differences from KV cache offloading:\n",
    "- Multiple replicas (2 pods)\n",
    "- Remote shared cache enabled\n",
    "- Redis backend for shared storage\n",
    "- Prefix caching enabled for better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up Redis for Shared Cache\n",
    "\n",
    "First, let's deploy Redis for our shared cache storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile redis-config.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: redis\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: redis\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: redis\n",
    "        image: redis:latest\n",
    "        ports:\n",
    "        - containerPort: 6379\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2\"\n",
    "          limits:\n",
    "            memory: \"8Gi\"\n",
    "            cpu: \"4\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: redis\n",
    "spec:\n",
    "  selector:\n",
    "    app: redis\n",
    "  ports:\n",
    "  - port: 6379\n",
    "    targetPort: 6379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl apply -f redis-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploying with Remote Shared KV Cache\n",
    "\n",
    "First, let's uninstall our previous deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm uninstall vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deploy with remote shared KV cache enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f shared-kv-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the logs to see if remote shared KV cache is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod names for the vLLM deployment\n",
    "!POD_NAMES=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}') && \\\n",
    "for pod in $POD_NAMES; do \\\n",
    "    echo \"\\nLogs for $pod:\" && \\\n",
    "    sudo microk8s kubectl logs $pod --tail=20; \\\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing Load Distribution\n",
    "\n",
    "Let's test how our setup handles requests across multiple pods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 53936:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# List of prompts to test with\n",
    "prompts = [\n",
    "    \"What are the key benefits of remote shared KV cache?\",\n",
    "    \"How does vLLM improve inference performance?\",\n",
    "    \"Explain the concept of KV cache offloading.\",\n",
    "    \"What are the best practices for LLM deployment?\"\n",
    "]\n",
    "\n",
    "def send_request(prompt):\n",
    "    url = \"http://localhost:53936/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return {\n",
    "        'latency': end_time - start_time,\n",
    "        'status': response.status_code,\n",
    "        'prompt': prompt\n",
    "    }\n",
    "\n",
    "# Test with multiple concurrent requests\n",
    "n_requests = 8\n",
    "print(f\"Sending {n_requests} concurrent requests...\\n\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=n_requests) as executor:\n",
    "    # Generate random prompts for testing\n",
    "    test_prompts = [random.choice(prompts) for _ in range(n_requests)]\n",
    "    results = list(executor.map(send_request, test_prompts))\n",
    "\n",
    "# Analyze results\n",
    "total_latency = sum(r['latency'] for r in results)\n",
    "avg_latency = total_latency / len(results)\n",
    "success_rate = sum(1 for r in results if r['status'] == 200) / len(results) * 100\n",
    "\n",
    "print(f\"Average latency: {avg_latency:.2f} seconds\")\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "print(f\"Total throughput: {n_requests / total_latency:.2f} requests/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing Cache Sharing\n",
    "\n",
    "Let's test if the KV cache is being shared effectively by sending similar prompts to different pods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base prompt that will be slightly modified for each request\n",
    "base_prompt = \"\"\"The development of artificial intelligence has been one of the most significant technological advances in recent history. \n",
    "It has transformed industries, enhanced scientific research, and changed how we interact with technology. \n",
    "From machine learning algorithms that power recommendation systems to natural language processing models that enable human-like conversations, \n",
    "AI continues to push the boundaries of what's possible.\"\"\"\n",
    "\n",
    "# List of questions to append to the base prompt\n",
    "questions = [\n",
    "    \"What are the key ethical considerations in AI development?\",\n",
    "    \"How can we ensure responsible AI development?\",\n",
    "    \"What are the main challenges in AI governance?\",\n",
    "    \"How can we address AI bias and fairness?\"\n",
    "]\n",
    "\n",
    "# Send requests with similar prompts\n",
    "results = []\n",
    "for question in questions:\n",
    "    prompt = f\"{base_prompt}\\n\\n{question}\"\n",
    "    result = send_request(prompt)\n",
    "    results.append(result)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Latency: {result['latency']:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "We've successfully implemented remote shared KV cache with Mistral-7B-Instruct-v0.3. Here are the key takeaways:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                       Key Takeaways                         │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Performance Impact  │      │ Resource Usage          │   │\n",
    "│  │ - Shared caching   │      │ - Distributed load      │   │\n",
    "│  │ - Better scaling   │      │ - Efficient storage     │   │\n",
    "│  │ - Load balancing   │      │ - Resource sharing      │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Best Use Cases      │      │ Considerations          │   │\n",
    "│  │ - High availability │      │ - Network latency       │   │\n",
    "│  │ - Multi-pod deploy  │      │ - Cache consistency     │   │\n",
    "│  │ - Fault tolerance   │      │ - Storage requirements  │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "When to use remote shared KV cache:\n",
    "- You need high availability and fault tolerance\n",
    "- You're running multiple vLLM instances\n",
    "- You want to optimize resource usage across pods\n",
    "- You need horizontal scaling capabilities\n",
    "\n",
    "Next, you can explore performance benchmarking (04_performance_benchmarking.ipynb) to measure and compare different configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}