{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KV Cache Offloading with vLLM and Mistral-7B-Instruct-v0.3\n",
    "\n",
    "This notebook demonstrates how to implement KV cache offloading in vLLM to optimize memory usage and improve performance for Mistral-7B-Instruct-v0.3.\n",
    "\n",
    "## What is KV Cache?\n",
    "\n",
    "The KV (Key-Value) cache is a critical component in transformer-based language models:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     KV Cache Overview                       │\n",
    "│                                                             │\n",
    "│  ┌─────────────┐       ┌─────────────┐      ┌──────────┐   │\n",
    "│  │ Input       │       │ Transformer │      │ Output   │   │\n",
    "│  │ Tokens      │─────►│ Layer       │─────►│ Tokens   │   │\n",
    "│  └─────────────┘       └─────┬───────┘      └──────────┘   │\n",
    "│                             │                              │\n",
    "│                             ▼                              │\n",
    "│                    ┌─────────────────┐                     │\n",
    "│                    │   KV Cache      │                     │\n",
    "│                    │ (GPU Memory)    │                     │\n",
    "│                    └─────────────────┘                     │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "- During generation, LLMs compute attention over previously generated tokens\n",
    "- The KV cache stores the key and value tensors from previous tokens\n",
    "- This speeds up inference but consumes a lot of GPU memory\n",
    "- Cache size grows linearly with sequence length\n",
    "\n",
    "## Why Offload KV Cache?\n",
    "\n",
    "KV cache offloading addresses several challenges:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 KV Cache Offloading Benefits                │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Memory Efficiency   │      │ Longer Contexts         │   │\n",
    "│  │ - Free GPU memory   │      │ - Handle long chats     │   │\n",
    "│  │ - Use for params    │      │ - Process documents     │   │\n",
    "│  │ - Better utilization│      │ - Maintain history      │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Cost Optimization   │      │ Higher Throughput       │   │\n",
    "│  │ - Use smaller GPUs  │      │ - More concurrent users │   │\n",
    "│  │ - Fewer GPUs needed │      │ - Better resource use   │   │\n",
    "│  │ - Lower TCO         │      │ - Faster responses      │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Recommended GPU VM Configuration\n",
    "\n",
    "For KV cache offloading with Mistral-7B-Instruct-v0.3, we recommend:\n",
    "\n",
    "- **GPU**: NVIDIA A10G or better (24GB+ VRAM)\n",
    "- **CPU**: 12+ cores (important for CPU offloading)\n",
    "- **RAM**: 48GB+ (critical for CPU offloading)\n",
    "- **Storage**: 100GB+ SSD\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up KV Cache Offloading\n",
    "\n",
    "First, let's create a configuration file that enables KV cache offloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile kv-offload-config.yaml\n",
    "servingEngineSpec:\n",
    "  modelSpec:\n",
    "  - name: \"mistral\"                      # Name for the deployment\n",
    "    repository: \"lmcache/vllm-openai\"    # Docker image with LMCache support\n",
    "    tag: \"latest\"                        # Image tag\n",
    "    modelURL: \"mistralai/Mistral-7B-Instruct-v0.3\"  # HuggingFace model ID\n",
    "    replicaCount: 1                      # Number of replicas to deploy\n",
    "    requestCPU: 12                       # CPU cores requested\n",
    "    requestMemory: \"48Gi\"                # Memory requested\n",
    "    requestGPU: 1                        # Number of GPUs requested\n",
    "    pvcStorage: \"50Gi\"                   # Persistent volume size\n",
    "    vllmConfig:                          # vLLM-specific configuration\n",
    "      enableChunkedPrefill: false        # Disable chunked prefill\n",
    "      enablePrefixCaching: false         # Disable prefix caching\n",
    "      maxModelLen: 16384                 # Maximum sequence length\n",
    "    \n",
    "    lmcacheConfig:                       # LMCache configuration\n",
    "      enabled: true                      # Enable LMCache\n",
    "      cpuOffloadingBufferSize: \"20\"      # 20GB of CPU memory for KV cache\n",
    "    \n",
    "    hf_token: \"\"                         # HuggingFace token (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Configuration\n",
    "\n",
    "Let's break down the key configuration parameters:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                Configuration Parameters                     │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Resource Config     │      │ Model Config            │   │\n",
    "│  │ - 12 CPU cores     │      │ - Mistral-7B-Instruct   │   │\n",
    "│  │ - 48GB RAM         │      │ - 16K context length     │   │\n",
    "│  │ - 1 GPU            │      │ - No prefix caching      │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ LMCache Config      │      │ Storage Config          │   │\n",
    "│  │ - Enabled          │      │ - 50GB PVC              │   │\n",
    "│  │ - 20GB CPU buffer  │      │ - Persistent storage    │   │\n",
    "│  │ - Auto-offloading  │      │ - Model checkpoints     │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Key differences from the basic setup:\n",
    "- Using `lmcache/vllm-openai` image for LMCache support\n",
    "- Increased CPU and memory resources for offloading\n",
    "- Enabled LMCache with 20GB CPU buffer\n",
    "- Increased maximum sequence length to 16K tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deploying with KV Cache Offloading\n",
    "\n",
    "First, let's uninstall our previous deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm uninstall vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deploy with KV cache offloading enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f kv-offload-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the logs to see if KV cache offloading is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod name for the vLLM deployment\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}') && \\\n",
    "sudo microk8s kubectl logs $POD_NAME --tail=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Long Context Performance\n",
    "\n",
    "Let's test how our setup handles long contexts by sending a request with a long prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 53936:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a long context by repeating a paragraph\n",
    "long_context = \"\"\"The development of artificial intelligence has been one of the most significant technological advances in recent history. \n",
    "It has transformed industries, enhanced scientific research, and changed how we interact with technology. \n",
    "From machine learning algorithms that power recommendation systems to natural language processing models that enable human-like conversations, \n",
    "AI continues to push the boundaries of what's possible. However, with these advances come important ethical considerations and \n",
    "responsibilities that we must carefully consider as we move forward.\\n\\n\"\" * 20\n",
    "\n",
    "# Send a request with the long context\n",
    "!curl -X POST http://localhost:53936/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d \"{\n",
    "    \\\"model\\\": \\\"mistralai/Mistral-7B-Instruct-v0.3\\\",\n",
    "    \\\"messages\\\": [\n",
    "      {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful AI assistant.\\\"}, \n",
    "      {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"$long_context\\nGiven this context about AI development, what are the three most important ethical considerations we should focus on? Please provide a concise answer.\\\"}\n",
    "    ],\n",
    "    \\\"temperature\\\": 0.7,\n",
    "    \\\"max_tokens\\\": 200\n",
    "  }\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monitoring Memory Usage\n",
    "\n",
    "Let's monitor the GPU and CPU memory usage during inference to see the effect of KV cache offloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU memory\n",
    "!nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu --format=csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor CPU memory\n",
    "!free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing Concurrent Requests\n",
    "\n",
    "Let's test how our setup handles multiple concurrent requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def send_request():\n",
    "    url = \"http://localhost:53936/v1/chat/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}, \n",
    "            {\"role\": \"user\", \"content\": \"What are three key benefits of KV cache offloading in LLM inference?\"}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "# Test with different numbers of concurrent requests\n",
    "concurrent_requests = [1, 2, 4, 8]\n",
    "latencies = []\n",
    "\n",
    "for n_requests in concurrent_requests:\n",
    "    print(f\"Testing with {n_requests} concurrent requests...\")\n",
    "    with ThreadPoolExecutor(max_workers=n_requests) as executor:\n",
    "        batch_latencies = list(executor.map(lambda _: send_request(), range(n_requests)))\n",
    "    \n",
    "    avg_latency = statistics.mean(batch_latencies)\n",
    "    latencies.append(avg_latency)\n",
    "    print(f\"Average latency: {avg_latency:.2f} seconds\\n\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(concurrent_requests, latencies, 'o-')\n",
    "plt.title('Latency vs Concurrent Requests')\n",
    "plt.xlabel('Number of Concurrent Requests')\n",
    "plt.ylabel('Average Latency (seconds)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "We've successfully implemented KV cache offloading with Mistral-7B-Instruct-v0.3. Here are the key takeaways:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                       Key Takeaways                         │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Performance Impact  │      │ Resource Usage          │   │\n",
    "│  │ - Longer contexts   │      │ - Lower GPU memory      │   │\n",
    "│  │ - More concurrent   │      │ - Higher CPU usage      │   │\n",
    "│  │   requests         │      │ - Better efficiency     │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Best Use Cases      │      │ Considerations          │   │\n",
    "│  │ - Long conversations│      │ - CPU memory needed     │   │\n",
    "│  │ - Document analysis │      │ - Latency trade-off     │   │\n",
    "│  │ - Multi-user serving│      │ - Resource balance      │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "When to use KV cache offloading:\n",
    "- You need to handle long conversations or documents\n",
    "- GPU memory is a bottleneck\n",
    "- You have sufficient CPU memory available\n",
    "- You want to serve more concurrent users\n",
    "\n",
    "Next, you can explore remote shared KV cache (03_remote_shared_kv_cache.ipynb) for even more advanced optimizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}