{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KV Cache Offloading with vLLM Production Stack\n",
    "\n",
    "This notebook demonstrates how to implement KV cache offloading in the vLLM production stack to optimize memory usage and improve performance for large language models.\n",
    "\n",
    "## What is KV Cache?\n",
    "\n",
    "The KV (Key-Value) cache is a critical component in transformer-based language models:\n",
    "\n",
    "- During generation, LLMs compute attention over previously generated tokens\n",
    "- The KV cache stores the key and value tensors from previous tokens to avoid recomputation\n",
    "- This significantly speeds up inference but consumes a lot of GPU memory\n",
    "- As conversations get longer, the KV cache grows linearly with the sequence length\n",
    "\n",
    "```\n",
    "┌───────────────────┐\n",
    "│     LLM Model     │\n",
    "└─────────┬─────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌───────────────────┐\n",
    "│    KV Cache in    │\n",
    "│    GPU Memory     │◄── Memory bottleneck for long sequences\n",
    "└───────────────────┘\n",
    "```\n",
    "\n",
    "## Why Offload KV Cache?\n",
    "\n",
    "KV cache offloading addresses several challenges:\n",
    "\n",
    "- **Memory Efficiency**: Frees up valuable GPU memory for model parameters\n",
    "- **Longer Contexts**: Enables processing of much longer conversations\n",
    "- **Cost Optimization**: Allows using smaller/fewer GPUs for the same workload\n",
    "- **Higher Throughput**: Serves more concurrent users with the same hardware\n",
    "\n",
    "## Recommended GPU VM Configuration\n",
    "\n",
    "For this tutorial, we recommend:\n",
    "\n",
    "- **GPU**: NVIDIA A10G or better (24GB+ VRAM)\n",
    "- **CPU**: 16+ cores (important for CPU offloading)\n",
    "- **RAM**: 64GB+ (critical for CPU offloading)\n",
    "- **Storage**: 100GB+ SSD\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before proceeding, make sure you have completed the setup in the first notebook (`01_setup_vllm_production_stack.ipynb`). Let's verify that our Kubernetes environment is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How CPU Offloading Works\n",
    "\n",
    "KV cache offloading to CPU works by moving the key-value tensors from GPU memory to CPU memory when they're not immediately needed, then bringing them back when required for inference.\n",
    "\n",
    "```\n",
    "┌───────────────────┐\n",
    "│     LLM Model     │\n",
    "└─────────┬─────────┘\n",
    "          │\n",
    "          ▼\n",
    "┌───────────────────┐         ┌───────────────────┐\n",
    "│  Active KV Cache  │◄───────►│ Offloaded KV Cache│\n",
    "│   (GPU Memory)    │         │   (CPU Memory)    │\n",
    "└───────────────────┘         └───────────────────┘\n",
    "```\n",
    "\n",
    "**Benefits of CPU Offloading:**\n",
    "- Allows handling longer sequences than would fit in GPU memory alone\n",
    "- Enables more concurrent requests with the same GPU resources\n",
    "- Provides a good balance between performance and memory efficiency\n",
    "\n",
    "We'll use LMCache, which integrates with vLLM to provide efficient KV cache offloading capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuring KV Cache Offloading to CPU\n",
    "\n",
    "Let's create a configuration file for KV cache offloading to CPU. We'll use Llama-2-7b-chat-hf as our model, which is a good balance between size and performance for demonstrating KV cache offloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cpu-offloading-config.yaml\n",
    "servingEngineSpec:\n",
    "  modelSpec:\n",
    "  - name: \"mistral\"                      # Name for the deployment\n",
    "    repository: \"lmcache/vllm-openai\"    # Docker image with LMCache support\n",
    "    tag: \"latest\"                        # Image tag\n",
    "    modelURL: \"mistralai/Mistral-7B-Instruct-v0.3\"  # HuggingFace model ID\n",
    "    replicaCount: 1                      # Number of replicas to deploy\n",
    "    requestCPU: 12                       # CPU cores requested\n",
    "    requestMemory: \"48Gi\"                # Memory requested\n",
    "    requestGPU: 1                        # Number of GPUs requested\n",
    "    pvcStorage: \"50Gi\"                   # Persistent volume size\n",
    "    vllmConfig:                          # vLLM-specific configuration\n",
    "      enableChunkedPrefill: false        # Disable chunked prefill\n",
    "      enablePrefixCaching: true          # Enable prefix caching\n",
    "      maxModelLen: 16384                 # Maximum sequence length\n",
    "    \n",
    "    lmcacheConfig:                       # LMCache configuration\n",
    "      enabled: true                      # Enable LMCache\n",
    "      cpuOffloadingBufferSize: \"20\"      # 20GB of CPU memory for KV cache\n",
    "    \n",
    "    hf_token: \"\"                         # HuggingFace token (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Configuration Parameters Explained\n",
    "\n",
    "- **repository**: We use `lmcache/vllm-openai` instead of the standard vLLM image to get LMCache support\n",
    "- **lmcacheConfig.enabled**: Enables the LMCache functionality\n",
    "- **cpuOffloadingBufferSize**: Specifies how much CPU memory (in GB) to allocate for KV cache offloading\n",
    "- **maxModelLen**: Sets the maximum sequence length; longer sequences benefit more from offloading\n",
    "\n",
    "Now let's deploy the vLLM stack with KV cache offloading to CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f cpu-offloading-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our deployment. This might take a few minutes as the model is downloaded and loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the logs to verify that LMCache is active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod name for the vLLM deployment\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep vllm-llama2-deployment | awk '{print $1}') && \\\n",
    "sudo microk8s kubectl logs $POD_NAME | grep -i lmcache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing KV Cache Offloading\n",
    "\n",
    "Let's test our deployment with KV cache offloading by sending a request with a long prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 30080:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API by listing available models\n",
    "!curl -o- http://localhost:30080/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a long prompt to test KV cache offloading. We'll create a conversation with multiple turns to build up a large KV cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_prompt = \"\"\"\n",
    "[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "Can you explain what KV cache is in transformer models? [/INST]\n",
    "\n",
    "In transformer models like those used in large language models (LLMs), the KV cache (Key-Value cache) is an optimization technique that significantly improves inference speed, especially during text generation.\n",
    "\n",
    "When a transformer generates text token by token, it needs to compute attention over all previous tokens for each new token. This involves calculating key (K) and value (V) vectors for each token through the model's layers. Without caching, these K and V vectors would need to be recomputed for all previous tokens every time a new token is generated, which is computationally expensive.\n",
    "\n",
    "The KV cache stores these key and value vectors after they're computed the first time, so they don't need to be recalculated for subsequent token generations. This makes the generation of each new token much faster after the first one.\n",
    "\n",
    "However, the KV cache does consume memory proportionally to the sequence length, which can become a limitation for very long contexts or when serving many concurrent requests.\n",
    "\n",
    "[INST] That's interesting! How much memory does the KV cache typically use? [/INST]\n",
    "\n",
    "The memory usage of the KV cache depends on several factors, but I can give you a general idea of how it scales:\n",
    "\n",
    "1. **Model size**: Larger models with more layers and wider attention heads use more memory for the KV cache.\n",
    "\n",
    "2. **Sequence length**: The KV cache grows linearly with the number of tokens in the sequence. This is why very long contexts can be challenging to handle.\n",
    "\n",
    "3. **Precision**: Whether the model is running in FP16, BF16, or FP32 precision affects memory usage.\n",
    "\n",
    "For a concrete example, let's consider a model like Llama 2 (7B) running with 16-bit precision:\n",
    "- Each key and value vector might use about 2 bytes per parameter\n",
    "- For each token in the sequence, the model stores K and V tensors for each layer\n",
    "- With 32 layers and a hidden size of 4096 (divided into 32 attention heads), the memory per token is substantial\n",
    "\n",
    "A rough calculation for a 7B parameter model with a 2048 token context might use around 1-2 GB just for the KV cache. For larger models like a 70B parameter model, this could be 10+ GB.\n",
    "\n",
    "This is why techniques like KV cache offloading (moving parts of the cache to CPU memory) or attention sinking (discarding some KV cache entries for tokens early in the context) are becoming important for efficient deployment.\n",
    "\n",
    "[INST] What are the main techniques for optimizing KV cache usage? [/INST]\n",
    "\"\"\"\n",
    "\n",
    "# Write the prompt to a file for easier use with curl\n",
    "with open(\"long_prompt.txt\", \"w\") as f:\n",
    "    f.write(long_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the completion endpoint with a long prompt\n",
    "!curl -X POST http://localhost:30080/v1/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d \"{\n",
    "    \\\"model\\\": \\\"mistralai/Mistral-7B-Instruct-v0.3\\\",\n",
    "    \\\"prompt\\\": $(cat long_prompt.txt | jq -Rs .),\n",
    "    \\\"max_tokens\\\": 500\n",
    "  }\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the logs again to see if we can observe KV cache offloading in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod name for the vLLM deployment\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep vllm-llama2-deployment | awk '{print $1}') && \\\n",
    "sudo microk8s kubectl logs $POD_NAME | grep -i \"cache\\|offload\" | tail -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmarking KV Cache Offloading\n",
    "\n",
    "Let's create a simple benchmark to test the performance of KV cache offloading. We'll send multiple requests with the same prompt to see how the KV cache offloading affects performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile benchmark.py\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def send_request(prompt, max_tokens=100):\n",
    "    \"\"\"Send a completion request to the vLLM API and measure response time\"\"\"\n",
    "    url = \"http://localhost:30080/v1/completions\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return {\n",
    "        \"status_code\": response.status_code,\n",
    "        \"response\": response.json() if response.status_code == 200 else None,\n",
    "        \"time\": end_time - start_time\n",
    "    }\n",
    "\n",
    "def run_benchmark(prompt, num_requests=5):\n",
    "    \"\"\"Run a benchmark with multiple identical requests to measure KV cache benefits\"\"\"\n",
    "    print(f\"Running benchmark with {num_requests} requests...\")\n",
    "    times = []\n",
    "    \n",
    "    for i in range(num_requests):\n",
    "        print(f\"Request {i+1}/{num_requests}\")\n",
    "        result = send_request(prompt)\n",
    "        if result[\"status_code\"] == 200:\n",
    "            times.append(result[\"time\"])\n",
    "            print(f\"Request {i+1} completed in {result['time']:.2f} seconds\")\n",
    "        else:\n",
    "            print(f\"Request {i+1} failed with status code {result['status_code']}\")\n",
    "    \n",
    "    if times:\n",
    "        print(\"\\nBenchmark Results:\")\n",
    "        print(f\"Average time: {statistics.mean(times):.2f} seconds\")\n",
    "        print(f\"Median time: {statistics.median(times):.2f} seconds\")\n",
    "        print(f\"Min time: {min(times):.2f} seconds\")\n",
    "        print(f\"Max time: {max(times):.2f} seconds\")\n",
    "        if len(times) > 1:\n",
    "            print(f\"Standard deviation: {statistics.stdev(times):.2f} seconds\")\n",
    "        \n",
    "        # Create a visualization of the results\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(range(1, len(times) + 1), times, marker='o', linestyle='-', color='blue')\n",
    "            plt.title('Request Latency Over Time (KV Cache Effect)')\n",
    "            plt.xlabel('Request Number')\n",
    "            plt.ylabel('Time (seconds)')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.savefig('kv_cache_benchmark.png')\n",
    "            print(\"\\nBenchmark visualization saved to 'kv_cache_benchmark.png'\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nCouldn't create visualization: {e}\")\n",
    "    else:\n",
    "        print(\"No successful requests to analyze.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Using a consistent prompt to demonstrate KV cache benefits\n",
    "    with open(\"long_prompt.txt\", \"r\") as f:\n",
    "        prompt = f.read()\n",
    "    \n",
    "    run_benchmark(prompt, num_requests=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install matplotlib if needed\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python benchmark.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the benchmark visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='kv_cache_benchmark.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Monitoring GPU Memory Usage\n",
    "\n",
    "Let's monitor the GPU memory usage during inference to see the effect of KV cache offloading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "When you're done, you can clean up the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm uninstall vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. Configured KV cache offloading to CPU using LMCache\n",
    "2. Deployed a Llama-2-7b model with KV cache offloading\n",
    "3. Tested the deployment with a long prompt\n",
    "4. Benchmarked the performance of KV cache offloading\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- KV cache offloading allows you to serve larger models and longer sequences with the same GPU resources\n",
    "- The performance impact of offloading is minimal for most use cases\n",
    "- CPU memory becomes an important resource when using KV cache offloading\n",
    "- This technique is particularly valuable for applications that require long conversations or document processing\n",
    "\n",
    "### When to Use KV Cache Offloading\n",
    "\n",
    "Consider implementing KV cache offloading when:\n",
    "- You need to support long conversations or documents\n",
    "- You want to maximize the number of concurrent users per GPU\n",
    "- You're working with limited GPU memory but have ample CPU memory\n",
    "- You need to scale horizontally while maintaining efficient resource usage\n",
    "\n",
    "In the next notebook, we'll explore remote shared KV cache storage, which takes offloading a step further by allowing multiple vLLM instances to share a common KV cache."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}