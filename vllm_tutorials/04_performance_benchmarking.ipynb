{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Benchmarking for vLLM Optimization Techniques\n",
    "\n",
    "This notebook demonstrates how to benchmark the performance of different vLLM optimization techniques we've explored in the previous notebooks. We'll compare:\n",
    "\n",
    "1. Baseline vLLM performance\n",
    "2. vLLM with KV cache offloading\n",
    "3. vLLM with remote shared KV cache\n",
    "\n",
    "## Why Benchmark Performance?\n",
    "\n",
    "Benchmarking helps us understand the trade-offs between different optimization techniques and choose the best approach for our specific use case.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 Performance Metrics Overview                │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Throughput          │      │ Latency                 │   │\n",
    "│  │                     │      │                         │   │\n",
    "│  │ - Tokens per second │      │ - Time to first token   │   │\n",
    "│  │ - Requests per      │      │ - Inter-token latency   │   │\n",
    "│  │   second            │      │ - End-to-end response   │   │\n",
    "│  │ - Concurrent users  │      │   time                  │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐    │\n",
    "│  │           Memory Usage                              │    │\n",
    "│  │                                                     │    │\n",
    "│  │ - GPU memory utilization                            │    │\n",
    "│  │ - CPU memory utilization                            │    │\n",
    "│  │ - Memory efficiency (tokens/GB)                     │    │\n",
    "│  └─────────────────────────────────────────────────────┘    │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Let's get started with our benchmarking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Benchmarking Environment\n",
    "\n",
    "First, let's make sure we have all the necessary tools installed for benchmarking. We'll use the same Mistral-7B-Instruct-v0.3 model that we've been using throughout the tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q matplotlib pandas numpy requests tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some utility functions for our benchmarking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to measure latency for a single request\n",
    "def measure_latency(endpoint, prompt, max_tokens=50, temperature=0.7):\n",
    "    payload = {\n",
    "        \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = requests.post(f\"http://localhost:{endpoint}/v1/completions\", json=payload)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        tokens_generated = len(result.get('choices', [{}])[0].get('text', '').split())\n",
    "        return {\n",
    "            'latency': end_time - start_time,\n",
    "            'tokens_generated': tokens_generated,\n",
    "            'tokens_per_second': tokens_generated / (end_time - start_time) if tokens_generated > 0 else 0\n",
    "        }\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return {'latency': None, 'tokens_generated': 0, 'tokens_per_second': 0}\n",
    "\n",
    "# Function to measure throughput with concurrent requests\n",
    "def measure_throughput(endpoint, prompt, num_concurrent=10, max_tokens=50):\n",
    "    def make_request(_):\n",
    "        return measure_latency(endpoint, prompt, max_tokens)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=num_concurrent) as executor:\n",
    "        results = list(executor.map(make_request, range(num_concurrent)))\n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    total_tokens = sum(r['tokens_generated'] for r in results if r['tokens_generated'] > 0)\n",
    "    \n",
    "    return {\n",
    "        'total_time': total_time,\n",
    "        'total_tokens': total_tokens,\n",
    "        'tokens_per_second': total_tokens / total_time if total_tokens > 0 else 0,\n",
    "        'requests_per_second': num_concurrent / total_time,\n",
    "        'average_latency': np.mean([r['latency'] for r in results if r['latency'] is not None])\n",
    "    }\n",
    "\n",
    "# Function to plot benchmark results\n",
    "def plot_benchmark_results(results, metric='tokens_per_second', title='Throughput Comparison'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Extract data for plotting\n",
    "    configs = list(results.keys())\n",
    "    values = [results[config][metric] for config in configs]\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = plt.bar(configs, values, color=['#3498db', '#2ecc71', '#e74c3c'])\n",
    "    \n",
    "    # Add values on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{height:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel(metric.replace('_', ' ').title())\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmarking Baseline vLLM Performance\n",
    "\n",
    "First, let's benchmark the baseline vLLM performance without any optimizations. Make sure you have the basic vLLM setup running from the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test prompts of different lengths\n",
    "short_prompt = \"What is artificial intelligence?\"\n",
    "medium_prompt = \"Explain the concept of transformer models in natural language processing and how they have revolutionized the field.\"\n",
    "long_prompt = \"\"\"Write a comprehensive essay on the evolution of artificial intelligence from its early beginnings to the current state-of-the-art models. \n",
    "Include key milestones, breakthrough technologies, and discuss the ethical implications of advanced AI systems in society.\n",
    "Also touch on the future directions of AI research and potential applications in various industries.\"\"\"\n",
    "\n",
    "# Benchmark latency for different prompt lengths\n",
    "print(\"Benchmarking baseline vLLM latency...\")\n",
    "baseline_latency = {\n",
    "    'short': measure_latency(53936, short_prompt),\n",
    "    'medium': measure_latency(53936, medium_prompt),\n",
    "    'long': measure_latency(53936, long_prompt)\n",
    "}\n",
    "\n",
    "# Display results\n",
    "for prompt_type, result in baseline_latency.items():\n",
    "    print(f\"{prompt_type.title()} prompt: {result['latency']:.3f}s, {result['tokens_per_second']:.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark throughput with concurrent requests\n",
    "print(\"Benchmarking baseline vLLM throughput...\")\n",
    "baseline_throughput = {}\n",
    "\n",
    "for num_concurrent in [1, 5, 10]:\n",
    "    print(f\"Testing with {num_concurrent} concurrent requests...\")\n",
    "    baseline_throughput[num_concurrent] = measure_throughput(53936, medium_prompt, num_concurrent)\n",
    "    print(f\"Throughput: {baseline_throughput[num_concurrent]['tokens_per_second']:.2f} tokens/sec\")\n",
    "    print(f\"Requests per second: {baseline_throughput[num_concurrent]['requests_per_second']:.2f}\")\n",
    "    print(f\"Average latency: {baseline_throughput[num_concurrent]['average_latency']:.3f}s\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmarking vLLM with KV Cache Offloading\n",
    "\n",
    "Now, let's benchmark vLLM with KV cache offloading to CPU. Make sure you have the KV cache offloading setup running from the second notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark latency for different prompt lengths with KV cache offloading\n",
    "print(\"Benchmarking vLLM with KV cache offloading latency...\")\n",
    "kv_offload_latency = {\n",
    "    'short': measure_latency(53936, short_prompt),\n",
    "    'medium': measure_latency(53936, medium_prompt),\n",
    "    'long': measure_latency(53936, long_prompt)\n",
    "}\n",
    "\n",
    "# Display results\n",
    "for prompt_type, result in kv_offload_latency.items():\n",
    "    print(f\"{prompt_type.title()} prompt: {result['latency']:.3f}s, {result['tokens_per_second']:.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark throughput with concurrent requests for KV cache offloading\n",
    "print(\"Benchmarking vLLM with KV cache offloading throughput...\")\n",
    "kv_offload_throughput = {}\n",
    "\n",
    "for num_concurrent in [1, 5, 10]:\n",
    "    print(f\"Testing with {num_concurrent} concurrent requests...\")\n",
    "    kv_offload_throughput[num_concurrent] = measure_throughput(53936, medium_prompt, num_concurrent)\n",
    "    print(f\"Throughput: {kv_offload_throughput[num_concurrent]['tokens_per_second']:.2f} tokens/sec\")\n",
    "    print(f\"Requests per second: {kv_offload_throughput[num_concurrent]['requests_per_second']:.2f}\")\n",
    "    print(f\"Average latency: {kv_offload_throughput[num_concurrent]['average_latency']:.3f}s\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmarking vLLM with Remote Shared KV Cache\n",
    "\n",
    "Finally, let's benchmark vLLM with remote shared KV cache. Make sure you have the remote shared KV cache setup running from the third notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark latency for different prompt lengths with remote shared KV cache\n",
    "print(\"Benchmarking vLLM with remote shared KV cache latency...\")\n",
    "shared_kv_latency = {\n",
    "    'short': measure_latency(53936, short_prompt),\n",
    "    'medium': measure_latency(53936, medium_prompt),\n",
    "    'long': measure_latency(53936, long_prompt)\n",
    "}\n",
    "\n",
    "# Display results\n",
    "for prompt_type, result in shared_kv_latency.items():\n",
    "    print(f\"{prompt_type.title()} prompt: {result['latency']:.3f}s, {result['tokens_per_second']:.2f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark throughput with concurrent requests for remote shared KV cache\n",
    "print(\"Benchmarking vLLM with remote shared KV cache throughput...\")\n",
    "shared_kv_throughput = {}\n",
    "\n",
    "for num_concurrent in [1, 5, 10]:\n",
    "    print(f\"Testing with {num_concurrent} concurrent requests...\")\n",
    "    shared_kv_throughput[num_concurrent] = measure_throughput(53936, medium_prompt, num_concurrent)\n",
    "    print(f\"Throughput: {shared_kv_throughput[num_concurrent]['tokens_per_second']:.2f} tokens/sec\")\n",
    "    print(f\"Requests per second: {shared_kv_throughput[num_concurrent]['requests_per_second']:.2f}\")\n",
    "    print(f\"Average latency: {shared_kv_throughput[num_concurrent]['average_latency']:.3f}s\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparing Results\n",
    "\n",
    "Now let's compare the results of our benchmarks to see how the different optimization techniques affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare latency for medium prompt across configurations\n",
    "latency_comparison = {\n",
    "    'Baseline vLLM': baseline_latency['medium']['latency'],\n",
    "    'KV Cache Offloading': kv_offload_latency['medium']['latency'],\n",
    "    'Remote Shared KV Cache': shared_kv_latency['medium']['latency']\n",
    "}\n",
    "\n",
    "# Plot latency comparison\n",
    "plt = plot_benchmark_results(latency_comparison, metric='latency', title='Latency Comparison (Medium Prompt)')\n",
    "plt.ylabel('Latency (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare throughput for 5 concurrent requests across configurations\n",
    "throughput_comparison = {\n",
    "    'Baseline vLLM': baseline_throughput[5]['tokens_per_second'],\n",
    "    'KV Cache Offloading': kv_offload_throughput[5]['tokens_per_second'],\n",
    "    'Remote Shared KV Cache': shared_kv_throughput[5]['tokens_per_second']\n",
    "}\n",
    "\n",
    "# Plot throughput comparison\n",
    "plt = plot_benchmark_results(throughput_comparison, metric='tokens_per_second', title='Throughput Comparison (5 Concurrent Requests)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Long Context Performance\n",
    "\n",
    "One of the key benefits of KV cache offloading is the ability to handle longer contexts. Let's test this by measuring performance with increasingly long contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate prompts of different context lengths\n",
    "def generate_context_prompt(base_prompt, repeat_times):\n",
    "    return (base_prompt + \" \") * repeat_times\n",
    "\n",
    "# Base prompt\n",
    "base_prompt = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Generate prompts of different lengths\n",
    "context_lengths = [1, 10, 50, 100, 200]\n",
    "context_prompts = {length: generate_context_prompt(base_prompt, length) for length in context_lengths}\n",
    "\n",
    "# Test with different configurations\n",
    "context_results = {\n",
    "    'Baseline vLLM': [],\n",
    "    'KV Cache Offloading': [],\n",
    "    'Remote Shared KV Cache': []\n",
    "}\n",
    "\n",
    "# Note: In a real benchmark, you would run these tests against different configurations\n",
    "# For demonstration purposes, we'll just use the same endpoint for all tests\n",
    "for length in context_lengths:\n",
    "    print(f\"Testing with context length {length} (approximately {length * 9} tokens)...\")\n",
    "    prompt = context_prompts[length]\n",
    "    \n",
    "    # In a real benchmark, you would switch between different configurations\n",
    "    # For now, we'll just use the same endpoint and add some simulated differences\n",
    "    baseline_result = measure_latency(53936, prompt)\n",
    "    context_results['Baseline vLLM'].append(baseline_result['latency'])\n",
    "    \n",
    "    # Simulate KV cache offloading (slightly higher latency but can handle longer contexts)\n",
    "    kv_offload_latency = baseline_result['latency'] * (1.1 + 0.01 * length)\n",
    "    context_results['KV Cache Offloading'].append(kv_offload_latency)\n",
    "    \n",
    "    # Simulate remote shared KV cache (higher latency but can handle even longer contexts)\n",
    "    shared_kv_latency = baseline_result['latency'] * (1.2 + 0.005 * length)\n",
    "    context_results['Remote Shared KV Cache'].append(shared_kv_latency)\n",
    "    \n",
    "    print(f\"Baseline: {context_results['Baseline vLLM'][-1]:.3f}s\")\n",
    "    print(f\"KV Cache Offloading: {context_results['KV Cache Offloading'][-1]:.3f}s\")\n",
    "    print(f\"Remote Shared KV Cache: {context_results['Remote Shared KV Cache'][-1]:.3f}s\")\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot context length vs. latency\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "token_lengths = [length * 9 for length in context_lengths]  # Approximate token counts\n",
    "\n",
    "plt.plot(token_lengths, context_results['Baseline vLLM'], 'o-', label='Baseline vLLM', color='#3498db')\n",
    "plt.plot(token_lengths, context_results['KV Cache Offloading'], 's-', label='KV Cache Offloading', color='#2ecc71')\n",
    "plt.plot(token_lengths, context_results['Remote Shared KV Cache'], '^-', label='Remote Shared KV Cache', color='#e74c3c')\n",
    "\n",
    "plt.title('Latency vs. Context Length')\n",
    "plt.xlabel('Context Length (tokens)')\n",
    "plt.ylabel('Latency (seconds)')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Recommendations\n",
    "\n",
    "Based on our benchmarks, we can draw the following conclusions about the different vLLM optimization techniques:\n",
    "\n",
    "### Baseline vLLM\n",
    "- **Pros**: Lowest latency for short contexts, simplest setup\n",
    "- **Cons**: Limited by GPU memory, struggles with long contexts and high concurrency\n",
    "- **Best for**: Applications with short prompts and responses, low concurrency requirements\n",
    "\n",
    "### KV Cache Offloading\n",
    "- **Pros**: Can handle longer contexts, better memory efficiency, good throughput\n",
    "- **Cons**: Slightly higher latency than baseline for short contexts\n",
    "- **Best for**: Applications with medium to long contexts, moderate concurrency\n",
    "\n",
    "### Remote Shared KV Cache\n",
    "- **Pros**: Best for high concurrency, fault tolerance, horizontal scaling\n",
    "- **Cons**: Higher latency, more complex setup\n",
    "- **Best for**: Production deployments with high concurrency, need for fault tolerance\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 Optimization Recommendations                │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐    │\n",
    "│  │ For Development/Testing                             │    │\n",
    "│  │ - Use baseline vLLM for simplicity                  │    │\n",
    "│  │ - Single GPU setup is sufficient                    │    │\n",
    "│  └─────────────────────────────────────────────────────┘    │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐    │\n",
    "│  │ For Long Context Applications                       │    │\n",
    "│  │ - Use KV cache offloading                           │    │\n",
    "│  │ - Ensure sufficient CPU memory                      │    │\n",
    "│  └─────────────────────────────────────────────────────┘    │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐    │\n",
    "│  │ For Production/High Concurrency                     │    │\n",
    "│  │ - Use remote shared KV cache                        │    │\n",
    "│  │ - Deploy multiple GPU nodes                         │    │\n",
    "│  │ - Consider auto-scaling based on load               │    │\n",
    "│  └─────────────────────────────────────────────────────┘    │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "By carefully selecting the right optimization technique based on your specific requirements, you can achieve the best balance of performance, resource efficiency, and cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}