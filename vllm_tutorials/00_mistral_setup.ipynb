{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up vLLM with Mistral-7B-Instruct-v0.3\n",
    "\n",
    "This notebook guides you through setting up a vLLM environment with the Mistral-7B-Instruct-v0.3 model. We'll cover:\n",
    "\n",
    "1. Setting up a Kubernetes environment with MicroK8s\n",
    "2. Installing the vLLM production stack using Helm\n",
    "3. Deploying and testing the Mistral-7B-Instruct-v0.3 model\n",
    "\n",
    "## What is vLLM?\n",
    "\n",
    "vLLM is a high-performance library for LLM inference and serving that significantly improves throughput and reduces latency.\n",
    "\n",
    "```\n",
    "┌───────────────────────────────────────────────────────────┐\n",
    "│                                                           │\n",
    "│                        vLLM Architecture                  │\n",
    "│                                                           │\n",
    "│  ┌─────────────┐       ┌─────────────┐      ┌──────────┐  │\n",
    "│  │ API Server  │◄─────►│ Scheduler   │◄────►│ Worker 1 │  │\n",
    "│  └─────────────┘       └─────────────┘      └──────────┘  │\n",
    "│         ▲                     ▲                  ▲        │\n",
    "│         │                     │                  │        │\n",
    "│         ▼                     ▼                  ▼        │\n",
    "│  ┌─────────────┐       ┌─────────────┐      ┌──────────┐  │\n",
    "│  │ Client      │       │ PagedAttn   │      │ Worker 2 │  │\n",
    "│  │ Applications│       │ Memory Mgmt │      └──────────┘  │\n",
    "│  └─────────────┘       └─────────────┘                    │\n",
    "│                                                           │\n",
    "└───────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- **PagedAttention**: Efficiently manages GPU memory by using a paging system\n",
    "- **Continuous Batching**: Dynamically processes requests without waiting for batch formation\n",
    "- **Optimized CUDA Kernels**: Custom kernels for faster execution\n",
    "- **OpenAI-compatible API**: Easy integration with existing applications\n",
    "\n",
    "## Recommended GPU VM Configuration\n",
    "\n",
    "For running Mistral-7B-Instruct-v0.3, we recommend:\n",
    "\n",
    "- **GPU**: NVIDIA A10G or better (24GB+ VRAM)\n",
    "- **CPU**: 8+ cores\n",
    "- **RAM**: 32GB+ (64GB recommended)\n",
    "- **Storage**: 100GB+ SSD\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Prerequisites\n",
    "\n",
    "### Understanding the Environment\n",
    "\n",
    "We'll be setting up a Kubernetes-based environment to run vLLM. Here's what our infrastructure will look like:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      Brev VM with GPU                       │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐    │\n",
    "│  │                   MicroK8s Cluster                  │    │\n",
    "│  │                                                     │    │\n",
    "│  │  ┌─────────────┐      ┌──────────────────────────┐  │    │\n",
    "│  │  │ Kubernetes  │      │      vLLM Pod            │  │    │\n",
    "│  │  │ Services    │◄────►│  ┌────────────────────┐  │  │    │\n",
    "│  │  │             │      │  │ Mistral-7B-Instruct│  │  │    │\n",
    "│  │  └─────────────┘      │  └────────────────────┘  │  │    │\n",
    "│  │                       └──────────────────────────┘  │    │\n",
    "│  │                                                     │    │\n",
    "│  └─────────────────────────────────────────────────────┘    │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                              ▲\n",
    "                              │\n",
    "                              ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      Local Machine                          │\n",
    "│                                                             │\n",
    "│  ┌─────────────┐      ┌──────────────────────────────────┐  │\n",
    "│  │ Brev CLI    │◄────►│ Browser/Client Application       │  │\n",
    "│  │ Port Forward│      │ (localhost:53936)                │  │\n",
    "│  └─────────────┘      └──────────────────────────────────┘  │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- **MicroK8s**: A lightweight Kubernetes distribution\n",
    "- **vLLM Pod**: Contains the Mistral-7B-Instruct-v0.3 model\n",
    "- **Brev CLI**: Used for port forwarding to access the model API\n",
    "\n",
    "First, let's check if we have GPU support available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Required Tools\n",
    "\n",
    "#### What is MicroK8s?\n",
    "\n",
    "MicroK8s is a lightweight, fast, and fully-conformant Kubernetes distribution that runs on just about any Linux box. It's perfect for:\n",
    "- Development environments\n",
    "- IoT devices\n",
    "- Edge computing\n",
    "- Single-node deployments like ours\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                       MicroK8s Setup                        │\n",
    "│                                                             │\n",
    "│  1. Install MicroK8s via snap                               │\n",
    "│  2. Add user to microk8s group                              │\n",
    "│  3. Enable GPU support                                      │\n",
    "│  4. Enable storage support                                  │\n",
    "│  5. Set up Helm repositories                                │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Let's create a script to install MicroK8s and set up our Kubernetes environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup_microk8s.sh\n",
    "#!/bin/bash\n",
    "# Function to check command status\n",
    "check_status() {\n",
    "    if [ $? -ne 0 ]; then\n",
    "        echo \"Error: $1 failed\"\n",
    "        exit 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Function to check if command exists\n",
    "command_exists() {\n",
    "    command -v \"$1\" >/dev/null 2>&1\n",
    "}\n",
    "\n",
    "# Check if nvidia-smi is available\n",
    "echo \"Checking NVIDIA GPU...\"\n",
    "if ! command_exists nvidia-smi; then\n",
    "    echo \"Warning: nvidia-smi not found. GPU support may not be available.\"\n",
    "else\n",
    "    nvidia-smi\n",
    "    check_status \"nvidia-smi\"\n",
    "fi\n",
    "\n",
    "# Install MicroK8s\n",
    "echo \"Installing MicroK8s...\"\n",
    "sudo snap install microk8s --classic --channel=1.25/stable\n",
    "check_status \"MicroK8s installation\"\n",
    "\n",
    "# Add user to microk8s group\n",
    "echo \"Adding user to microk8s group...\"\n",
    "sudo usermod -a -G microk8s $USER\n",
    "check_status \"Adding user to microk8s group\"\n",
    "\n",
    "# Create and set permissions for .kube directory\n",
    "echo \"Setting up .kube directory...\"\n",
    "mkdir -p ~/.kube\n",
    "chmod 0700 ~/.kube\n",
    "sudo chown -f -R $USER ~/.kube\n",
    "check_status \"Setting up .kube directory\"\n",
    "\n",
    "# Wait for MicroK8s to be ready\n",
    "echo \"Waiting for MicroK8s to be ready...\"\n",
    "sudo microk8s status --wait-ready\n",
    "check_status \"MicroK8s ready check\"\n",
    "\n",
    "# Enable GPU and storage support\n",
    "echo \"Enabling GPU and hostpath-storage...\"\n",
    "sudo microk8s enable gpu hostpath-storage\n",
    "check_status \"Enabling MicroK8s addons\"\n",
    "\n",
    "# Double check status\n",
    "echo \"Checking final MicroK8s status...\"\n",
    "sudo microk8s status --wait-ready\n",
    "check_status \"Final MicroK8s status check\"\n",
    "\n",
    "# Set up Helm repositories\n",
    "echo \"Setting up Helm repositories...\"\n",
    "sudo microk8s helm repo remove nvidia || true  # Remove if exists\n",
    "sudo microk8s helm repo add nvidia https://helm.ngc.nvidia.com/nvidia\n",
    "sudo microk8s helm repo update\n",
    "\n",
    "# Activate the new group membership without requiring logout\n",
    "echo \"Activating microk8s group membership...\"\n",
    "if ! groups | grep -q microk8s; then\n",
    "    exec sg microk8s -c '\n",
    "        echo \"Testing cluster access...\"\n",
    "        sudo microk8s kubectl get services\n",
    "        sudo microk8s kubectl get nodes\n",
    "        echo \"Creating example nginx deployment...\"\n",
    "        sudo microk8s kubectl create deployment nginx --image=nginx\n",
    "        echo \"Checking pods...\"\n",
    "        sudo microk8s kubectl get pods\n",
    "        echo \"The kubectl and helm aliases are now active globally.\"\n",
    "    '\n",
    "else\n",
    "    echo \"Testing cluster access...\"\n",
    "    sudo microk8s kubectl get services\n",
    "    sudo microk8s kubectl get nodes\n",
    "    echo \"Creating example nginx deployment...\"\n",
    "    sudo microk8s kubectl create deployment nginx --image=nginx\n",
    "    echo \"Checking pods...\"\n",
    "    sudo microk8s kubectl get pods\n",
    "fi\n",
    "\n",
    "echo \"Setup of MicroK8s is complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the script executable and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x setup_microk8s.sh\n",
    "!./setup_microk8s.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up vLLM with Helm\n",
    "\n",
    "### What is Helm?\n",
    "\n",
    "Helm is a package manager for Kubernetes that simplifies the deployment and management of applications. Think of it as:\n",
    "- npm for Node.js\n",
    "- pip for Python\n",
    "- apt for Ubuntu\n",
    "\n",
    "But specifically designed for Kubernetes applications.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     Helm Deployment Flow                    │\n",
    "│                                                             │\n",
    "│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐  │\n",
    "│  │ Helm Chart  │─────►│ Kubernetes  │─────►│ Running     │  │\n",
    "│  │ (vLLM)      │      │ API Server  │      │ Application │  │\n",
    "│  └─────────────┘      └─────────────┘      └─────────────┘  │\n",
    "│         ▲                                                   │\n",
    "│         │                                                   │\n",
    "│  ┌─────────────┐                                            │\n",
    "│  │ Values File │                                            │\n",
    "│  │ (YAML)      │                                            │\n",
    "│  └─────────────┘                                            │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "The vLLM Helm chart will deploy:\n",
    "1. A router service for load balancing and request distribution\n",
    "2. vLLM engine pods that run the actual model\n",
    "3. Necessary configurations for GPU access and networking\n",
    "\n",
    "Now that we have our Kubernetes environment ready, let's set up vLLM using Helm. First, we'll add the vLLM Helm repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the vLLM Helm repository\n",
    "!sudo microk8s helm repo add vllm https://vllm-project.github.io/production-stack\n",
    "!sudo microk8s helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploying Mistral-7B-Instruct-v0.3\n",
    "\n",
    "### About Mistral-7B-Instruct-v0.3\n",
    "\n",
    "Mistral-7B-Instruct-v0.3 is a powerful instruction-tuned language model that offers a good balance between performance and resource requirements. It's an excellent choice for:\n",
    "- Text generation\n",
    "- Question answering\n",
    "- Summarization\n",
    "- Chat applications\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 Mistral-7B-Instruct-v0.3                    │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Model Architecture  │      │ Key Specifications      │   │\n",
    "│  │ - Transformer-based │      │ - 7 billion parameters  │   │\n",
    "│  │ - Sliding Window    │      │ - Context: 8K tokens    │   │\n",
    "│  │ - Grouped Query     │      │ - Instruction-tuned     │   │\n",
    "│  │   Attention (GQA)   │      │ - ~13GB in half precision│   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Model Loading Process

vLLM uses a different approach from standard HuggingFace pipelines for loading models. Let's compare:

```
┌─────────────────────────────────────────────────────────────┐
│                   Model Loading Approaches                   │
│                                                             │
│  ┌─────────────────────┐      ┌─────────────────────────┐   │
│  │ HuggingFace Pipeline│      │ vLLM Loading            │   │
│  │ - Simple but slow   │      │ - Optimized for speed   │   │
│  │ - More memory      │      │ - PagedAttention        │   │
│  │ - Single GPU       │      │ - Multi-GPU support     │   │
│  └─────────────────────┘      └─────────────────────────┘   │
│                                                             │
│  Standard Pipeline:            vLLM Equivalent:             │
│  ```python                     ```python                    │
│  from transformers import      from vllm import LLM,        │
│    pipeline                      SamplingParams             │
│                                                            │
│  pipe = pipeline(             llm = LLM(model=             │
│    "text-generation",           "mistral-7b")              │
│    model="mistral-7b")                                     │
│  ```                          ```                          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

Before we deploy the model, let's understand how vLLM loads the Mistral-7B-Instruct-v0.3 weights:

```
┌─────────────────────────────────────────────────────────────┐
│                   Model Loading Process                     │
│                                                             │
│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐  │
│  │ HuggingFace │      │ Local Cache │      │ GPU Memory  │  │
│  │ Repository  │─────►│ (PVC)       │─────►│ (CUDA)      │  │
│  └─────────────┘      └─────────────┘      └─────────────┘  │
│         ▲                    ▲                   ▲          │
│         │                    │                   │          │
│         ▼                    ▼                   ▼          │
│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐  │
│  │ Download    │      │ Tensor      │      │ Engine      │  │
│  │ Manager     │      │ Parallelism │      │ Workers     │  │
│  └─────────────┘      └─────────────┘      └─────────────┘  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

1. **Download and Initialization Process**:
   ```python
   # What happens inside vLLM (simplified)
   from vllm.model_executor import ModelExecutor
   from vllm.model_executor.parallel_utils import initialize_model_parallel
   
   # Initialize model parallel environment
   initialize_model_parallel()
   
   # Load model weights efficiently
   executor = ModelExecutor(
       model_name="mistralai/Mistral-7B-Instruct-v0.3",
       tensor_parallel_size=1,  # Number of GPUs for tensor parallelism
       dtype="half",           # Use FP16 for efficiency
   )
   ```
   
   - vLLM uses HuggingFace's `safetensors` format for efficient loading
   - Model files are downloaded to persistent volume (PVC)
   - Cached for future use to avoid re-downloading

2. **Weight Loading and Optimization**:
   ```python
   # Inside vLLM's ModelExecutor (simplified)
   def load_weights(self):
       # Load model architecture
       config = AutoConfig.from_pretrained(self.model_name)
       
       # Load weights with optimizations
       weights = load_tensor_parallel_weights(
           model_name=self.model_name,
           num_gpus=self.tensor_parallel_size,
           dtype=self.dtype,
       )
       
       # Initialize PagedAttention
       self.init_paged_attention(
           max_num_sequences=self.max_num_sequences,
           block_size=self.block_size,
       )
   ```
   
   - Model architecture is loaded first (config.json)
   - Weights are loaded in chunks to manage memory
   - Automatic FP16/BF16 conversion for efficiency
   - PagedAttention initialization for memory optimization

3. **GPU Allocation and Memory Management**:
   ```python
   # Inside vLLM's CacheEngine (simplified)
   class CacheEngine:
       def __init__(self):
           # Initialize CUDA streams for async operations
           self.cache_stream = torch.cuda.Stream()
           self.copy_stream = torch.cuda.Stream()
           
           # Initialize PagedAttention blocks
           self.gpu_blocks = PagedAttentionBuffer(
               block_size=self.block_size,
               num_blocks=self.num_gpu_blocks,
               dtype=self.dtype,
           )
           
           # Initialize CPU cache if enabled
           if self.cpu_offload:
               self.cpu_blocks = PagedAttentionBuffer(
                   block_size=self.block_size,
                   num_blocks=self.num_cpu_blocks,
                   dtype=self.dtype,
                   in_cpu=True,
               )
   ```
   
   - Weights are moved to GPU using CUDA streams
   - Tensor parallelism for multi-GPU setups
   - PagedAttention for efficient memory management
   - Optional CPU offloading for longer sequences

4. **Worker Initialization and Serving**:
   ```python
   # Inside vLLM's AsyncLLMEngine (simplified)
   class AsyncLLMEngine:
       def __init__(self):
           # Initialize workers
           self.workers = [
               ModelWorker(
                   model_config=self.model_config,
                   cache_config=self.cache_config,
                   parallel_config=self.parallel_config,
               )
               for _ in range(self.num_workers)
           ]
           
           # Initialize scheduler
           self.scheduler = Scheduler(
               schedulable_workers=self.workers,
               cache_config=self.cache_config,
           )
           
           # Start serving
           self.server = AsyncServer(
               engine=self,
               port=self.port,
               host=self.host,
           )
   ```
   
   - vLLM engine workers are started
   - Each worker gets a GPU context
   - KV cache is initialized
   - Scheduler manages request distribution
   - Server handles API endpoints

Now let's create a configuration file for deploying the Mistral-7B-Instruct-v0.3 model. This YAML file defines how the model will be deployed in our Kubernetes cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mistral-config.yaml\n",
    "servingEngineSpec:\n",
    "  runtimeClassName: \"\"                  # Runtime class name (leave empty for default)\n",
    "  modelSpec:\n",
    "  - name: \"mistral\"                     # Name for the deployment\n",
    "    repository: \"vllm/vllm-openai\"      # Docker image for vLLM\n",
    "    tag: \"latest\"                       # Image tag\n",
    "    modelURL: \"mistralai/Mistral-7B-Instruct-v0.3\"  # Mistral model\n",
    "    hf_token: \"\"                        # Your HuggingFace token (if needed)\n",
    "    replicaCount: 1                     # Single replica\n",
    "    requestCPU: 8                       # CPU cores requested\n",
    "    requestMemory: \"32Gi\"               # Memory requested\n",
    "    requestGPU: 1                       # Number of GPUs requested\n",
    "    pvcStorage: \"50Gi\"                  # Persistent volume size\n",
    "    vllmConfig:                         # vLLM-specific configuration\n",
    "      enableChunkedPrefill: false       # Disable chunked prefill\n",
    "      enablePrefixCaching: true         # Enable prefix caching\n",
    "      maxModelLen: 8192                 # Maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deploy the Mistral model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f mistral-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting Model Loading

Common issues during model loading and their solutions:

```
┌─────────────────────────────────────────────────────────────┐
│                   Common Loading Issues                     │
│                                                             │
│  ┌─────────────────────┐      ┌─────────────────────────┐   │
│  │ Download Issues     │      │ Memory Issues           │   │
│  │ - Network timeout   │      │ - Out of GPU memory     │   │
│  │ - HF auth error    │      │ - CUDA OOM error        │   │
│  │ - Disk space full  │      │ - Worker crash          │   │
│  └─────────────────────┘      └─────────────────────────┘   │
│                                                             │
│  ┌─────────────────────┐      ┌─────────────────────────┐   │
│  │ Format Issues       │      │ Worker Issues           │   │
│  │ - Wrong precision   │      │ - GPU not detected      │   │
│  │ - Corrupt weights   │      │ - CUDA version mismatch │   │
│  │ - Missing files     │      │ - Driver issues         │   │
│  └─────────────────────┘      └─────────────────────────┘   │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**Solutions:**
1. **Download Issues**:
   - Check network connectivity
   - Verify HuggingFace token if needed
   - Ensure sufficient disk space

2. **Memory Issues**:
   - Use half precision (FP16/BF16)
   - Increase GPU memory
   - Enable gradient checkpointing

3. **Format Issues**:
   - Clear the model cache
   - Re-download weights
   - Check model compatibility

4. **Worker Issues**:
   - Update NVIDIA drivers
   - Check CUDA compatibility
   - Verify GPU access

Let's check the status of our deployment. This might take a few minutes as the model is downloaded and loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Model Loading

Let's check the logs to see the progress of the model loading. The process typically follows these stages:

```
┌─────────────────────────────────────────────────────────────┐
│                   Model Loading Stages                      │
│                                                             │
│  1. ┌─────────────────────┐                                 │
│     │ Download Weights    │ ~5-10 minutes                   │
│     └─────────────────────┘                                 │
│                                                             │
│  2. ┌─────────────────────┐                                 │
│     │ Convert Format     │ ~2-3 minutes                    │
│     └─────────────────────┘                                 │
│                                                             │
│  3. ┌─────────────────────┐                                 │
│     │ Load to GPU        │ ~1-2 minutes                    │
│     └─────────────────────┘                                 │
│                                                             │
│  4. ┌─────────────────────┐                                 │
│     │ Initialize Workers │ ~30 seconds                     │
│     └─────────────────────┘                                 │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

You can monitor these stages in the pod logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod name and check logs\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}') && \\\n",
    "echo \"=== Pod Logs ===\" && \\\n",
    "sudo microk8s kubectl logs $POD_NAME --tail=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Resource Usage\n",
    "\n",
    "During model loading, it's important to monitor GPU memory usage and verify that model files are properly downloaded:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                   Resource Monitoring                       │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ GPU Memory Usage    │      │ Storage Usage           │   │\n",
    "│  │ - Model weights    │      │ - Model files           │   │\n",
    "│  │ - KV cache        │      │ - Safetensors           │   │\n",
    "│  │ - CUDA buffers    │      │ - Configuration         │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  Expected Memory Usage for Mistral-7B:                      │\n",
    "│  - Model weights: ~13GB (FP16)                             │\n",
    "│  - CUDA buffers: ~2GB                                      │\n",
    "│  - KV cache: varies with batch size and sequence length    │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU memory usage\n",
    "!echo \"=== GPU Memory Usage ===\" && \\\n",
    "nvidia-smi --query-gpu=memory.used,memory.total,utilization.gpu --format=csv,noheader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model files in PVC\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}') && \\\n",
    "echo \"=== Model Files in PVC ===\" && \\\n",
    "sudo microk8s kubectl exec $POD_NAME -- ls -lh /data/models/mistralai/Mistral-7B-Instruct-v0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Deployment\n",
    "\n",
    "### Setting Up Port Forwarding\n",
    "\n",
    "To access our vLLM service from outside the Kubernetes cluster, we need to set up port forwarding. There are two ways to do this:\n",
    "\n",
    "#### Option 1: Using kubectl port-forward (for local testing)\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Port Forwarding Flow                     │\n",
    "│                                                             │\n",
    "│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐  │\n",
    "│  │ Local       │      │ kubectl     │      │ Kubernetes  │  │\n",
    "│  │ Port 53936  │─────►│ port-forward│─────►│ Service     │  │\n",
    "│  └─────────────┘      └─────────────┘      └─────────────┘  │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 53936:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Using Brev CLI (for remote access)\n",
    "\n",
    "If you're running this on a Brev.dev instance, you can use the Brev CLI for port forwarding. This allows you to access the service from your local machine.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 Brev Port Forwarding Flow                   │\n",
    "│                                                             │\n",
    "│  ┌─────────────┐      ┌─────────────┐      ┌─────────────┐  │\n",
    "│  │ Local       │      │ Brev CLI    │      │ Remote      │  │\n",
    "│  │ Machine     │─────►│ port-forward│─────►│ Brev VM     │  │\n",
    "│  └─────────────┘      └─────────────┘      └─────────────┘  │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "From your local machine, you would run:\n",
    "```bash\n",
    "brev port-forward your-workspace-name --port 53936:53936\n",
    "```\n",
    "\n",
    "Or interactively:\n",
    "```bash\n",
    "brev port-forward your-workspace-name\n",
    "# Then enter 53936 for both remote and local ports when prompted\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API by listing available models\n",
    "!curl -o- http://localhost:53936/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the completion endpoint with Mistral\n",
    "!curl -X POST http://localhost:53936/v1/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"prompt\": \"Write a short poem about artificial intelligence.\",\n",
    "    \"max_tokens\": 150,\n",
    "    \"temperature\": 0.7\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Chat Endpoint\n",
    "\n",
    "Let's also test the chat endpoint, which is more appropriate for instruction-tuned models like Mistral-7B-Instruct-v0.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://localhost:53936/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Explain how vLLM improves LLM inference performance in 3 bullet points.\"}\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 200\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "You've successfully set up a vLLM environment with the Mistral-7B-Instruct-v0.3 model! This environment is now ready for the optimization techniques covered in the subsequent notebooks.\n",
    "\n",
    "### Optimization Techniques Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 vLLM Optimization Techniques                │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ KV Cache Offloading │      │ Remote Shared KV Cache  │   │\n",
    "│  │ (Notebook 02)       │      │ (Notebook 03)           │   │\n",
    "│  │                     │      │                         │   │\n",
    "│  │ - Offloads KV cache │      │ - Shares KV cache       │   │\n",
    "│  │   to CPU memory     │      │   across multiple pods  │   │\n",
    "│  │ - Enables longer    │      │ - Improves fault        │   │\n",
    "│  │   context windows   │      │   tolerance             │   │\n",
    "│  │ - Reduces GPU       │      │ - Enables horizontal    │   │\n",
    "│  │   memory pressure   │      │   scaling               │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐    │\n",
    "│  │           Performance Benchmarking                  │    │\n",
    "│  │           (Notebook 04)                             │    │\n",
    "│  │                                                     │    │\n",
    "│  │ - Measures throughput and latency                   │    │\n",
    "│  │ - Compares different optimization techniques        │    │\n",
    "│  │ - Helps identify optimal configurations             │    │\n",
    "│  └─────────────────────────────────────────────────────┘    │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **KV Cache Offloading** (02_kv_cache_offloading.ipynb)\n",
    "   - Offloads key-value cache from GPU to CPU memory\n",
    "   - Enables processing of much longer conversations\n",
    "   - Frees up valuable GPU memory for model parameters\n",
    "\n",
    "2. **Remote Shared KV Cache** (03_remote_shared_kv_cache.ipynb)\n",
    "   - Allows multiple vLLM instances to share a common KV cache\n",
    "   - Improves resource efficiency and fault tolerance\n",
    "   - Enables horizontal scaling without duplicating cached data\n",
    "\n",
    "3. **Performance Benchmarking** (04_performance_benchmarking.ipynb)\n",
    "   - Tests the performance of different configurations\n",
    "   - Measures throughput, latency, and memory usage\n",
    "   - Helps identify the optimal setup for your use case\n",
    "\n",
    "These techniques will help you optimize the performance of your LLM inference pipeline and get the most out of your GPU resources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}