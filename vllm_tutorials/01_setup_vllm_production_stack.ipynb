{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up vLLM Production Stack with Mistral-7B-Instruct-v0.3\n",
    "\n",
    "This notebook guides you through setting up a vLLM production stack with Mistral-7B-Instruct-v0.3. We'll cover:\n",
    "\n",
    "1. Setting up a Kubernetes environment with MicroK8s\n",
    "2. Installing the vLLM production stack using Helm\n",
    "3. Deploying and testing Mistral-7B-Instruct-v0.3\n",
    "\n",
    "## What is vLLM?\n",
    "\n",
    "vLLM is a high-performance library for LLM inference and serving. It's designed to maximize throughput and minimize latency for LLM applications.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                        vLLM Architecture                    │\n",
    "│                                                             │\n",
    "│  ┌─────────────┐       ┌─────────────┐      ┌──────────┐   │\n",
    "│  │ API Server  │◄─────►│ Scheduler   │◄────►│ Worker 1 │   │\n",
    "│  └─────────────┘       └─────────────┘      └──────────┘   │\n",
    "│         ▲                     ▲                  ▲         │\n",
    "│         │                     │                  │         │\n",
    "│         ▼                     ▼                  ▼         │\n",
    "│  ┌─────────────┐       ┌─────────────┐      ┌──────────┐   │\n",
    "│  │ Client      │       │ PagedAttn   │      │ Worker 2 │   │\n",
    "│  │ Applications│       │ Memory Mgmt │      └──────────┘   │\n",
    "│  └─────────────┘       └─────────────┘                     │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Key features include:\n",
    "- PagedAttention for efficient memory management\n",
    "- Continuous batching to handle concurrent requests\n",
    "- Optimized CUDA kernels for faster execution\n",
    "- OpenAI-compatible API for easy integration\n",
    "\n",
    "## Recommended GPU VM Configuration\n",
    "\n",
    "For running Mistral-7B-Instruct-v0.3, we recommend:\n",
    "\n",
    "- **GPU**: NVIDIA A10G or better (24GB+ VRAM)\n",
    "- **CPU**: 8+ cores\n",
    "- **RAM**: 32GB+ (64GB recommended)\n",
    "- **Storage**: 100GB+ SSD\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Prerequisites\n",
    "\n",
    "### What is Kubernetes and why use it for LLM deployment?\n",
    "\n",
    "Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. For LLM deployments, Kubernetes offers several advantages:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                 Kubernetes Benefits for LLMs                │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ Scalability        │      │ Resource Management      │   │\n",
    "│  │ - Auto-scaling     │      │ - GPU allocation        │   │\n",
    "│  │ - Load balancing   │      │ - Memory limits         │   │\n",
    "│  │ - Rolling updates  │      │ - Resource quotas       │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────┐      ┌─────────────────────────┐   │\n",
    "│  │ High Availability  │      │ Infrastructure as Code   │   │\n",
    "│  │ - Pod replication  │      │ - Declarative configs   │   │\n",
    "│  │ - Self-healing     │      │ - Version control       │   │\n",
    "│  │ - Node failover    │      │ - Easy replication      │   │\n",
    "│  └─────────────────────┘      └─────────────────────────┘   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "First, let's check if we have GPU support available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Required Tools\n",
    "\n",
    "Let's create a script to install MicroK8s and set up our Kubernetes environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup_microk8s.sh\n",
    "#!/bin/bash\n",
    "# Function to check command status\n",
    "check_status() {\n",
    "    if [ $? -ne 0 ]; then\n",
    "        echo \"Error: $1 failed\"\n",
    "        exit 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Function to check if command exists\n",
    "command_exists() {\n",
    "    command -v \"$1\" >/dev/null 2>&1\n",
    "}\n",
    "\n",
    "# Check if nvidia-smi is available\n",
    "echo \"Checking NVIDIA GPU...\"\n",
    "if ! command_exists nvidia-smi; then\n",
    "    echo \"Warning: nvidia-smi not found. GPU support may not be available.\"\n",
    "else\n",
    "    nvidia-smi\n",
    "    check_status \"nvidia-smi\"\n",
    "fi\n",
    "\n",
    "# Install MicroK8s\n",
    "echo \"Installing MicroK8s...\"\n",
    "sudo snap install microk8s --classic --channel=1.25/stable\n",
    "check_status \"MicroK8s installation\"\n",
    "\n",
    "# Add user to microk8s group\n",
    "echo \"Adding user to microk8s group...\"\n",
    "sudo usermod -a -G microk8s $USER\n",
    "check_status \"Adding user to microk8s group\"\n",
    "\n",
    "# Create and set permissions for .kube directory\n",
    "echo \"Setting up .kube directory...\"\n",
    "mkdir -p ~/.kube\n",
    "chmod 0700 ~/.kube\n",
    "sudo chown -f -R $USER ~/.kube\n",
    "check_status \"Setting up .kube directory\"\n",
    "\n",
    "# Wait for MicroK8s to be ready\n",
    "echo \"Waiting for MicroK8s to be ready...\"\n",
    "sudo microk8s status --wait-ready\n",
    "check_status \"MicroK8s ready check\"\n",
    "\n",
    "# Enable GPU and storage support\n",
    "echo \"Enabling GPU and hostpath-storage...\"\n",
    "sudo microk8s enable gpu hostpath-storage\n",
    "check_status \"Enabling MicroK8s addons\"\n",
    "\n",
    "# Double check status\n",
    "echo \"Checking final MicroK8s status...\"\n",
    "sudo microk8s status --wait-ready\n",
    "check_status \"Final MicroK8s status check\"\n",
    "\n",
    "# Set up Helm repositories\n",
    "echo \"Setting up Helm repositories...\"\n",
    "sudo microk8s helm repo remove nvidia || true  # Remove if exists\n",
    "sudo microk8s helm repo add nvidia https://helm.ngc.nvidia.com/nvidia\n",
    "sudo microk8s helm repo update\n",
    "\n",
    "# Activate the new group membership without requiring logout\n",
    "echo \"Activating microk8s group membership...\"\n",
    "if ! groups | grep -q microk8s; then\n",
    "    exec sg microk8s -c '\n",
    "        echo \"Testing cluster access...\"\n",
    "        sudo microk8s kubectl get services\n",
    "        sudo microk8s kubectl get nodes\n",
    "        echo \"Creating example nginx deployment...\"\n",
    "        sudo microk8s kubectl create deployment nginx --image=nginx\n",
    "        echo \"Checking pods...\"\n",
    "        sudo microk8s kubectl get pods\n",
    "        echo \"The kubectl and helm aliases are now active globally.\"\n",
    "    '\n",
    "else\n",
    "    echo \"Testing cluster access...\"\n",
    "    sudo microk8s kubectl get services\n",
    "    sudo microk8s kubectl get nodes\n",
    "    echo \"Creating example nginx deployment...\"\n",
    "    sudo microk8s kubectl create deployment nginx --image=nginx\n",
    "    echo \"Checking pods...\"\n",
    "    sudo microk8s kubectl get pods\n",
    "fi\n",
    "\n",
    "echo \"Setup of MicroK8s is complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the script executable and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x setup_microk8s.sh\n",
    "!./setup_microk8s.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up vLLM with Helm\n",
    "\n",
    "Now that we have our Kubernetes environment ready, let's set up vLLM using Helm. First, we'll add the vLLM Helm repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the vLLM Helm repository\n",
    "!sudo microk8s helm repo add vllm https://vllm-project.github.io/production-stack\n",
    "!sudo microk8s helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploying Mistral-7B-Instruct-v0.3\n",
    "\n",
    "Now let's create a configuration file for deploying the Mistral-7B-Instruct-v0.3 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mistral-config.yaml\n",
    "servingEngineSpec:\n",
    "  runtimeClassName: \"\"                  # Runtime class name (leave empty for default)\n",
    "  modelSpec:\n",
    "  - name: \"mistral\"                     # Name for the deployment\n",
    "    repository: \"vllm/vllm-openai\"      # Docker image for vLLM\n",
    "    tag: \"latest\"                       # Image tag\n",
    "    modelURL: \"mistralai/Mistral-7B-Instruct-v0.3\"  # Mistral model\n",
    "    hf_token: \"\"                        # Your HuggingFace token (if needed)\n",
    "    replicaCount: 1                     # Single replica\n",
    "    requestCPU: 8                       # CPU cores requested\n",
    "    requestMemory: \"32Gi\"               # Memory requested\n",
    "    requestGPU: 1                       # Number of GPUs requested\n",
    "    pvcStorage: \"50Gi\"                  # Persistent volume size\n",
    "    vllmConfig:                         # vLLM-specific configuration\n",
    "      enableChunkedPrefill: false       # Disable chunked prefill\n",
    "      enablePrefixCaching: true         # Enable prefix caching\n",
    "      maxModelLen: 8192                 # Maximum sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deploy the Mistral model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f mistral-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our deployment. This might take a few minutes as the model is downloaded and loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the logs to see the progress of the model loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod name for the vLLM deployment\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}') && \\\n",
    "sudo microk8s kubectl logs $POD_NAME --tail=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Testing the Deployment\n",
    "\n",
    "Once the pod is running, let's test our deployment by forwarding the service port and sending a request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 53936:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API by listing available models\n",
    "!curl -o- http://localhost:53936/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the completion endpoint\n",
    "!curl -X POST http://localhost:53936/v1/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"prompt\": \"Write a short poem about artificial intelligence.\",\n",
    "    \"max_tokens\": 150,\n",
    "    \"temperature\": 0.7\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Chat Endpoint\n",
    "\n",
    "Let's also test the chat endpoint, which is more appropriate for instruction-tuned models like Mistral-7B-Instruct-v0.3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://localhost:53936/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "      {\"role\": \"user\", \"content\": \"Explain how vLLM improves LLM inference performance in 3 bullet points.\"}\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": 200\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "Now that you have a basic vLLM deployment working with Mistral-7B-Instruct-v0.3, you can explore advanced optimization techniques:\n",
    "\n",
    "1. **KV Cache Offloading** (02_kv_cache_offloading.ipynb)\n",
    "   - Offload key-value cache to CPU memory\n",
    "   - Handle longer sequences\n",
    "   - Optimize GPU memory usage\n",
    "\n",
    "2. **Remote Shared KV Cache** (03_remote_shared_kv_cache.ipynb)\n",
    "   - Share KV cache across multiple instances\n",
    "   - Improve fault tolerance\n",
    "   - Enable horizontal scaling\n",
    "\n",
    "3. **Performance Benchmarking** (04_performance_benchmarking.ipynb)\n",
    "   - Measure throughput and latency\n",
    "   - Compare different configurations\n",
    "   - Optimize for your use case\n",
    "\n",
    "These techniques will help you get the most out of your GPU resources and optimize your LLM deployment for production use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}