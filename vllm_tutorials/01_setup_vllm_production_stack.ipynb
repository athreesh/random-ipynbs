{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up vLLM Production Stack on Brev.dev\n",
    "\n",
    "This notebook guides you through setting up a vLLM production stack on brev.dev. We'll cover:\n",
    "\n",
    "1. Setting up a Kubernetes environment with MicroK8s\n",
    "2. Installing the vLLM production stack using Helm\n",
    "3. Testing the deployment with a simple model\n",
    "\n",
    "## What is vLLM?\n",
    "\n",
    "vLLM is a high-performance library for LLM inference and serving. It's designed to maximize throughput and minimize latency for LLM applications. Key features include:\n",
    "\n",
    "- PagedAttention for efficient memory management\n",
    "- Continuous batching to handle concurrent requests\n",
    "- Optimized CUDA kernels for faster execution\n",
    "- OpenAI-compatible API for easy integration\n",
    "\n",
    "## Recommended GPU VM Configuration\n",
    "\n",
    "For this tutorial, we recommend the following VM configuration:\n",
    "\n",
    "- **GPU**: NVIDIA A10G or better (24GB+ VRAM)\n",
    "- **CPU**: 8+ cores\n",
    "- **RAM**: 32GB+ (64GB recommended)\n",
    "- **Storage**: 100GB+ SSD\n",
    "\n",
    "While you can run vLLM on smaller GPUs (like T4 with 16GB VRAM), having more VRAM allows you to load larger models and handle more concurrent requests.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Prerequisites\n",
    "\n",
    "### What is Kubernetes and why use it for LLM deployment?\n",
    "\n",
    "Kubernetes is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. For LLM deployments, Kubernetes offers several advantages:\n",
    "\n",
    "- **Scalability**: Easily scale your LLM services up or down based on demand\n",
    "- **Resource Management**: Efficiently allocate GPU and CPU resources across workloads\n",
    "- **High Availability**: Ensure your LLM services remain available even if individual components fail\n",
    "- **Declarative Configuration**: Define your entire LLM infrastructure as code\n",
    "\n",
    "We'll use MicroK8s, a lightweight Kubernetes distribution that's easy to set up and use on a single machine.\n",
    "\n",
    "First, let's check if we have GPU support available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Install Required Tools\n",
    "\n",
    "Let's create a script to install MicroK8s and set up our Kubernetes environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup_microk8s.sh\n",
    "#!/bin/bash\n",
    "# Function to check command status\n",
    "check_status() {\n",
    "    if [ $? -ne 0 ]; then\n",
    "        echo \"Error: $1 failed\"\n",
    "        exit 1\n",
    "    fi\n",
    "}\n",
    "\n",
    "# Function to check if command exists\n",
    "command_exists() {\n",
    "    command -v \"$1\" >/dev/null 2>&1\n",
    "}\n",
    "\n",
    "# Check if nvidia-smi is available\n",
    "echo \"Checking NVIDIA GPU...\"\n",
    "if ! command_exists nvidia-smi; then\n",
    "    echo \"Warning: nvidia-smi not found. GPU support may not be available.\"\n",
    "else\n",
    "    nvidia-smi\n",
    "    check_status \"nvidia-smi\"\n",
    "fi\n",
    "\n",
    "# Install MicroK8s\n",
    "echo \"Installing MicroK8s...\"\n",
    "sudo snap install microk8s --classic --channel=1.25/stable\n",
    "check_status \"MicroK8s installation\"\n",
    "\n",
    "# Add user to microk8s group\n",
    "echo \"Adding user to microk8s group...\"\n",
    "sudo usermod -a -G microk8s $USER\n",
    "check_status \"Adding user to microk8s group\"\n",
    "\n",
    "# Create and set permissions for .kube directory\n",
    "echo \"Setting up .kube directory...\"\n",
    "mkdir -p ~/.kube\n",
    "chmod 0700 ~/.kube\n",
    "sudo chown -f -R $USER ~/.kube\n",
    "check_status \"Setting up .kube directory\"\n",
    "\n",
    "# Wait for MicroK8s to be ready\n",
    "echo \"Waiting for MicroK8s to be ready...\"\n",
    "sudo microk8s status --wait-ready\n",
    "check_status \"MicroK8s ready check\"\n",
    "\n",
    "# Enable GPU and storage support\n",
    "echo \"Enabling GPU and hostpath-storage...\"\n",
    "sudo microk8s enable gpu hostpath-storage\n",
    "check_status \"Enabling MicroK8s addons\"\n",
    "\n",
    "# Double check status\n",
    "echo \"Checking final MicroK8s status...\"\n",
    "sudo microk8s status --wait-ready\n",
    "check_status \"Final MicroK8s status check\"\n",
    "\n",
    "# Set up Helm repositories\n",
    "echo \"Setting up Helm repositories...\"\n",
    "sudo microk8s helm repo remove nvidia || true  # Remove if exists\n",
    "sudo microk8s helm repo add nvidia https://helm.ngc.nvidia.com/nvidia\n",
    "sudo microk8s helm repo update\n",
    "\n",
    "# Activate the new group membership without requiring logout\n",
    "echo \"Activating microk8s group membership...\"\n",
    "if ! groups | grep -q microk8s; then\n",
    "    exec sg microk8s -c '\n",
    "        echo \"Testing cluster access...\"\n",
    "        sudo microk8s kubectl get services\n",
    "        sudo microk8s kubectl get nodes\n",
    "        echo \"Creating example nginx deployment...\"\n",
    "        sudo microk8s kubectl create deployment nginx --image=nginx\n",
    "        echo \"Checking pods...\"\n",
    "        sudo microk8s kubectl get pods\n",
    "        echo \"The kubectl and helm aliases are now active globally.\"\n",
    "    '\n",
    "else\n",
    "    echo \"Testing cluster access...\"\n",
    "    sudo microk8s kubectl get services\n",
    "    sudo microk8s kubectl get nodes\n",
    "    echo \"Creating example nginx deployment...\"\n",
    "    sudo microk8s kubectl create deployment nginx --image=nginx\n",
    "    echo \"Checking pods...\"\n",
    "    sudo microk8s kubectl get pods\n",
    "fi\n",
    "\n",
    "echo \"Setup of MicroK8s is complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the script executable and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x setup_microk8s.sh\n",
    "!./setup_microk8s.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Clone the vLLM Production Stack Repository\n",
    "\n",
    "Let's clone the vLLM production stack repository to get access to the necessary configuration files and examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/vllm-project/production-stack.git\n",
    "!cd production-stack && ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Minimal vLLM Installation\n",
    "\n",
    "### Understanding Helm and vLLM Stack\n",
    "\n",
    "Helm is a package manager for Kubernetes that simplifies the deployment and management of applications. Think of it as npm for Node.js or pip for Python, but for Kubernetes applications.\n",
    "\n",
    "The vLLM production stack consists of several components:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│                 Client Request                  │\n",
    "└───────────────────────┬─────────────────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│                Router Service                   │\n",
    "│  (Load balancing and request distribution)      │\n",
    "└───────────────────────┬─────────────────────────┘\n",
    "                        │\n",
    "                        ▼\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│               vLLM Engine Pods                  │\n",
    "│  (Model serving with GPU acceleration)          │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Let's start with a minimal installation of vLLM to ensure everything is working correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the vLLM Helm repository\n",
    "!sudo microk8s helm repo add vllm https://vllm-project.github.io/production-stack\n",
    "!sudo microk8s helm repo update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a minimal configuration file for our initial deployment. We'll use Mistral-7B-Instruct-v0.3, which is a powerful yet efficient model for our testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile minimal-vllm-config.yaml\n",
    "servingEngineSpec:\n",
    "  runtimeClassName: \"\"                  # Runtime class name (leave empty for default)\n",
    "  modelSpec:\n",
    "  - name: \"mistral\"                     # Name for the deployment\n",
    "    repository: \"vllm/vllm-openai\"      # Docker image for vLLM\n",
    "    tag: \"latest\"                       # Image tag\n",
    "    modelURL: \"mistralai/Mistral-7B-Instruct-v0.3\"  # Mistral model
    hf_token: \"\"                        # Your HuggingFace token (if needed)\n",
    "    replicaCount: 1                     # Single replica\n",
    "    requestCPU: 8                       # CPU cores requested\n",
    "    requestMemory: \"32Gi\"               # Memory requested\n",
    "    requestGPU: 1                       # Number of GPUs requested"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deploy the minimal vLLM stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f minimal-vllm-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods\n",
    "!sudo microk8s kubectl get services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our deployment by forwarding the service port and sending a request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 30080:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API by listing available models\n",
    "!curl -o- http://localhost:30080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the completion endpoint\n",
    "!curl -X POST http://localhost:30080/v1/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"facebook/opt-125m\",\n",
    "    \"prompt\": \"Once upon a time,\",\n",
    "    \"max_tokens\": 50\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploying a Larger Model with Hugging Face\n",
    "\n",
    "Now that we have a basic setup working, let's deploy a larger model from Hugging Face. For this example, we'll use Mistral-7B-Instruct-v0.2, which is a good balance between size and performance.\n",
    "\n",
    "First, let's uninstall our previous deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm uninstall vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Setting Up Hugging Face Access\n",
    "\n",
    "Some models on Hugging Face require authentication. If you need to use a token, you can get one from your [Hugging Face account settings](https://huggingface.co/settings/tokens).\n",
    "\n",
    "For this example, we'll use Mistral-7B-Instruct-v0.2, which is openly available and doesn't require a token. However, we'll include the token field in our configuration for future reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mistral-config.yaml\n",
    "servingEngineSpec:\n",
    "  modelSpec:\n",
    "  - name: \"mistral\"                      # Name for the deployment\n",
    "    repository: \"vllm/vllm-openai\"       # Docker image for vLLM\n",
    "    tag: \"latest\"                        # Image tag\n",
    "    modelURL: \"mistralai/Mistral-7B-Instruct-v0.3\"  # HuggingFace model ID\n",
    "    replicaCount: 1                      # Number of replicas to deploy\n",
    "    requestCPU: 8                        # CPU cores requested\n",
    "    requestMemory: \"32Gi\"                # Memory requested\n",
    "    requestGPU: 1                        # Number of GPUs requested\n",
    "    pvcStorage: \"50Gi\"                   # Persistent volume size\n",
    "    vllmConfig:                          # vLLM-specific configuration\n",
    "      enableChunkedPrefill: false        # Disable chunked prefill\n",
    "      enablePrefixCaching: true          # Enable prefix caching\n",
    "      maxModelLen: 8192                  # Maximum sequence length\n",
    "    \n",
    "    hf_token: \"\"                         # HuggingFace token (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's deploy the Mistral model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm install vllm vllm/vllm-stack -f mistral-config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of our deployment. This might take a few minutes as the model is downloaded and loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the logs to see the progress of the model loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pod name for the vLLM deployment\n",
    "!POD_NAME=$(sudo microk8s kubectl get pods | grep vllm-mistral-deployment | awk '{print $1}') && \\\n",
    "sudo microk8s kubectl logs $POD_NAME --tail=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the pod is running, let's test our deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will run in the background\n",
    "!sudo microk8s kubectl port-forward svc/vllm-router-service 30080:80 > port_forward.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait a moment for the port forwarding to establish\n",
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the API by listing available models\n",
    "!curl -o- http://localhost:30080/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the completion endpoint with Mistral\n",
    "!curl -X POST http://localhost:30080/v1/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"prompt\": \"Explain the concept of attention in transformer models:\",\n",
    "    \"max_tokens\": 150\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleanup\n",
    "\n",
    "When you're done, you can clean up the deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo microk8s helm uninstall vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've successfully:\n",
    "\n",
    "1. Set up a Kubernetes environment with MicroK8s\n",
    "2. Installed the vLLM production stack using Helm\n",
    "3. Deployed and tested a small model (OPT-125M)\n",
    "4. Deployed and tested a larger model (Mistral-7B)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Now that you have a basic vLLM production stack running, you can explore more advanced features:\n",
    "\n",
    "- **KV Cache Offloading**: See the next notebook for details on how to optimize memory usage with KV cache offloading\n",
    "- **Multi-Model Deployment**: Deploy multiple models in the same cluster\n",
    "- **Performance Tuning**: Adjust vLLM parameters for optimal performance\n",
    "- **Scaling**: Add more replicas to handle higher load\n",
    "\n",
    "Check out the other notebooks in this series for more advanced topics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
